{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Toxicity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a version of the toxicity model that uses attention. It uses a CNN architecture for text classification trained on the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and pre-trained GloVe embeddings which can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "This model is a modification of [example code](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) found in the [Keras Github repository](https://github.com/fchollet/keras) and released under an [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE). For further details of this license, find it [online](https://github.com/fchollet/keras/blob/master/LICENSE) or in this repository in the file KERAS_LICENSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "Prior to running the notebook, you must:\n",
    "\n",
    "* Download the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)\n",
    "* Download pre-trained [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "* (optional) To skip the training step, you will need to download a model and tokenizer file. We are looking into the appropriate means for distributing these (sometimes large) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model_tool import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model_tool import (DEFAULT_EMBEDDINGS_PATH,\n",
    "DEFAULT_MODEL_DIR, DEFAULT_HPARAMS)\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Multiply\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "\n",
    "class AttentionToxModel(ToxModel):\n",
    "    def __init__(self,\n",
    "                 model_name=None,\n",
    "                 model_dir=DEFAULT_MODEL_DIR,\n",
    "                 embeddings_path=DEFAULT_EMBEDDINGS_PATH,\n",
    "                 hparams=None):\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.hparams = DEFAULT_HPARAMS.copy()\n",
    "        self.embeddings_path = embeddings_path\n",
    "        if hparams:\n",
    "            self.update_hparams(hparams)\n",
    "        if model_name:\n",
    "            self.load_model_from_name(model_name)\n",
    "            self.load_probs_model(model_name)\n",
    "        self.print_hparams()\n",
    "\n",
    "    def load_probs_model(self, model_name):\n",
    "        probs_model_name = model_name + \"_probs\"\n",
    "        self.probs_model = load_model(\n",
    "                os.path.join(\n",
    "                    self.model_dir, '%s_model.h5' % probs_model_name))\n",
    "\n",
    "    def save_prob_model(self):\n",
    "        self.probs_model_name = self.model_name + \"probs\"\n",
    "\n",
    "    def build_dense_attention_layer(self, input_tensor):\n",
    "        # softmax\n",
    "        attention_probs = Dense(self.hparams['max_sequence_length'],\n",
    "                                activation='softmax',\n",
    "                                name='attention_vec')(input_tensor)\n",
    "        # context vector\n",
    "        attention_mul = Multiply()([input_tensor, attention_probs])\n",
    "        return {'attention_probs': attention_probs,\n",
    "                'attention_preds': attention_mul}\n",
    "\n",
    "    def train(\n",
    "                self,\n",
    "                training_data_path,\n",
    "                validation_data_path, text_column,\n",
    "                label_column, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.save_hparams(model_name)\n",
    "\n",
    "        train_data = pd.read_csv(training_data_path)\n",
    "        valid_data = pd.read_csv(validation_data_path)\n",
    "\n",
    "        print('Fitting tokenizer...')\n",
    "        self.fit_and_save_tokenizer(train_data[text_column])\n",
    "        print('Tokenizer fitted!')\n",
    "\n",
    "        print('Preparing data...')\n",
    "        train_text, train_labels = (self.prep_text(train_data[text_column]),\n",
    "                                    to_categorical(train_data[label_column]))\n",
    "        valid_text, valid_labels = (self.prep_text(valid_data[text_column]),\n",
    "                                    to_categorical(valid_data[label_column]))\n",
    "        print('Data prepared!')\n",
    "\n",
    "        print('Loading embeddings...')\n",
    "        self.load_embeddings()\n",
    "        print('Embeddings loaded!')\n",
    "\n",
    "        print('Building model graph...')\n",
    "        self.build_model()\n",
    "        print('Training model...')\n",
    "        tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()), write_graph=True)\n",
    "\n",
    "        preds_save_path = os.path.join(\n",
    "                            self.model_dir, '%s_model.h5' % self.model_name)\n",
    "        probs_save_path = os.path.join(\n",
    "                            self.model_dir, '%s_probs_model.h5'\n",
    "                            % self.model_name)\n",
    "        preds_callbacks = [ModelCheckpoint(\n",
    "                            preds_save_path,\n",
    "                            save_best_only=True,\n",
    "                            verbose=self.hparams['verbose']), tensorboard]\n",
    "        probs_callbacks = [ModelCheckpoint(\n",
    "                            probs_save_path,\n",
    "                            save_best_only=True,\n",
    "                            verbose=self.hparams['verbose'])]\n",
    "\n",
    "        if self.hparams['stop_early']:\n",
    "            early_stop = EarlyStopping(\n",
    "                            min_delta=self.hparams['es_min_delta'],\n",
    "                            monitor='val_loss',\n",
    "                            patience=self.hparams['es_patience'],\n",
    "                            verbose=self.hparams['verbose'], mode='auto')\n",
    "            probs_callbacks.append(early_stop)\n",
    "            preds_callbacks.append(early_stop)\n",
    "\n",
    "        self.model.fit(train_text,\n",
    "                       train_labels,\n",
    "                       batch_size=self.hparams['batch_size'],\n",
    "                       epochs=self.hparams['epochs'],\n",
    "                       validation_data=(valid_text, valid_labels),\n",
    "                       callbacks=preds_callbacks,\n",
    "                       verbose=2)\n",
    "\n",
    "        print('Model trained!')\n",
    "        print('Best model saved to {}'.format(preds_save_path))\n",
    "        print('Fitting probs model')\n",
    "\n",
    "        self.probs_model.fit(\n",
    "                    train_text,\n",
    "                    train_labels,\n",
    "                    batch_size=self.hparams['batch_size'],\n",
    "                    epochs=self.hparams['epochs'],\n",
    "                    validation_data=(valid_text, valid_labels),\n",
    "                    callbacks=probs_callbacks,\n",
    "                    verbose=2)\n",
    "        self.probs_model = load_model(probs_save_path)\n",
    "        print('Loading best model from checkpoint...')\n",
    "        self.model = load_model(preds_save_path)\n",
    "        print('Model loaded!')\n",
    "\n",
    "    def build_model(self):\n",
    "        print('print inside build model')\n",
    "        sequence_input = Input(\n",
    "                            shape=(self.hparams['max_sequence_length'],),\n",
    "                            dtype='int32')\n",
    "        embedding_layer = Embedding(\n",
    "                            len(self.tokenizer.word_index) + 1,\n",
    "                            self.hparams['embedding_dim'],\n",
    "                            weights=[self.embedding_matrix],\n",
    "                            input_length=self.hparams['max_sequence_length'],\n",
    "                            trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        for filter_size, kernel_size, pool_size in zip(\n",
    "                self.hparams['cnn_filter_sizes'],\n",
    "                self.hparams['cnn_kernel_sizes'],\n",
    "                self.hparams['cnn_pooling_sizes']):\n",
    "            x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'], name=\"Dropout\")(x)\n",
    "        x = Dense(self.hparams['max_sequence_length'], activation='relu', name=\"Dense_RELU\")(x)\n",
    "\n",
    "        # build prediction model\n",
    "        attention_dict = self.build_dense_attention_layer(x)\n",
    "        preds = attention_dict['attention_preds']\n",
    "        preds = Dense(2, name=\"preds_dense\", activation='softmax')(preds)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer=rmsprop,\n",
    "                metrics=['acc'])\n",
    "\n",
    "        # now make probs model\n",
    "        probs = attention_dict['attention_probs']\n",
    "        probs = Dense(2, name='probs_dense')(probs)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.probs_model = Model(sequence_input, preds)\n",
    "        self.probs_model.compile(\n",
    "                loss='mse', optimizer=rmsprop, metrics=['acc'])\n",
    "        # build probabilities model\n",
    "        self.save_prob_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams = {'epochs': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22855, saving model to ../models/test_atn_cnn_random_tox_v4_100_model.h5\n",
      "11s - loss: 0.2938 - acc: 0.9055 - val_loss: 0.2286 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22855 to 0.18390, saving model to ../models/test_atn_cnn_random_tox_v4_100_model.h5\n",
      "10s - loss: 0.2086 - acc: 0.9202 - val_loss: 0.1839 - val_acc: 0.9347\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2a8ed4d75458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrandom_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionToxModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrandom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-ece764251adc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data_path, validation_data_path, text_column, label_column, model_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                        verbose=2)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model trained!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jb/repos/unintended-ml-bias-analysis/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_names = ['test_atn_cnn_random_tox_v4_{}'.format(i) for i in xrange(100, 110)]\n",
    "for model_name in model_names:\n",
    "    MODEL_NAME = model_name\n",
    "    random_model = AttentionToxModel(hparams=hparams)\n",
    "    random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.963784838550671"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain attention wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24035, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "12s - loss: 0.3070 - acc: 0.9032 - val_loss: 0.2403 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24035 to 0.19245, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.2164 - acc: 0.9149 - val_loss: 0.1925 - val_acc: 0.9316\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19245 to 0.16974, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1835 - acc: 0.9344 - val_loss: 0.1697 - val_acc: 0.9389\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16974 to 0.16152, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1660 - acc: 0.9397 - val_loss: 0.1615 - val_acc: 0.9418\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16152 to 0.15200, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1545 - acc: 0.9435 - val_loss: 0.1520 - val_acc: 0.9449\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15200 to 0.14566, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1450 - acc: 0.9470 - val_loss: 0.1457 - val_acc: 0.9467\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14566 to 0.13770, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1380 - acc: 0.9494 - val_loss: 0.1377 - val_acc: 0.9489\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13770 to 0.13612, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1309 - acc: 0.9518 - val_loss: 0.1361 - val_acc: 0.9505\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13612 to 0.12849, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1262 - acc: 0.9541 - val_loss: 0.1285 - val_acc: 0.9520\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12849 to 0.12730, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1212 - acc: 0.9559 - val_loss: 0.1273 - val_acc: 0.9520\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12730 to 0.12331, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1168 - acc: 0.9580 - val_loss: 0.1233 - val_acc: 0.9541\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12331 to 0.12232, saving model to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "11s - loss: 0.1129 - acc: 0.9592 - val_loss: 0.1223 - val_acc: 0.9543\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "11s - loss: 0.1092 - acc: 0.9607 - val_loss: 0.1303 - val_acc: 0.9489\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "11s - loss: 0.1047 - acc: 0.9624 - val_loss: 0.1342 - val_acc: 0.9461\n",
      "Epoch 00013: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_100_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03743, saving model to ../models/atn_cnn_wiki_tox_v4_100_probs_model.h5\n",
      "11s - loss: 0.0283 - acc: 0.9633 - val_loss: 0.0374 - val_acc: 0.9481\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03743 to 0.03465, saving model to ../models/atn_cnn_wiki_tox_v4_100_probs_model.h5\n",
      "11s - loss: 0.0270 - acc: 0.9653 - val_loss: 0.0347 - val_acc: 0.9551\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03465 to 0.03431, saving model to ../models/atn_cnn_wiki_tox_v4_100_probs_model.h5\n",
      "11s - loss: 0.0259 - acc: 0.9669 - val_loss: 0.0343 - val_acc: 0.9547\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "11s - loss: 0.0248 - acc: 0.9686 - val_loss: 0.0448 - val_acc: 0.9384\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.0240 - acc: 0.9699 - val_loss: 0.0347 - val_acc: 0.9561\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23836, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "12s - loss: 0.3020 - acc: 0.9034 - val_loss: 0.2384 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23836 to 0.20550, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.2172 - acc: 0.9105 - val_loss: 0.2055 - val_acc: 0.9306\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20550 to 0.17916, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1900 - acc: 0.9321 - val_loss: 0.1792 - val_acc: 0.9368\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17916 to 0.16177, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1723 - acc: 0.9383 - val_loss: 0.1618 - val_acc: 0.9423\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.1566 - acc: 0.9420 - val_loss: 0.1807 - val_acc: 0.9253\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.16177 to 0.16049, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1448 - acc: 0.9460 - val_loss: 0.1605 - val_acc: 0.9346\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.16049 to 0.14573, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1366 - acc: 0.9492 - val_loss: 0.1457 - val_acc: 0.9429\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14573 to 0.13383, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1298 - acc: 0.9518 - val_loss: 0.1338 - val_acc: 0.9468\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "11s - loss: 0.1241 - acc: 0.9538 - val_loss: 0.1395 - val_acc: 0.9462\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.13383 to 0.13019, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1192 - acc: 0.9562 - val_loss: 0.1302 - val_acc: 0.9496\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "11s - loss: 0.1146 - acc: 0.9578 - val_loss: 0.1453 - val_acc: 0.9441\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.13019 to 0.12328, saving model to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "11s - loss: 0.1113 - acc: 0.9591 - val_loss: 0.1233 - val_acc: 0.9539\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "11s - loss: 0.1067 - acc: 0.9612 - val_loss: 0.1296 - val_acc: 0.9494\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "11s - loss: 0.1027 - acc: 0.9627 - val_loss: 0.1238 - val_acc: 0.9528\n",
      "Epoch 00013: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_101_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03580, saving model to ../models/atn_cnn_wiki_tox_v4_101_probs_model.h5\n",
      "12s - loss: 0.0276 - acc: 0.9638 - val_loss: 0.0358 - val_acc: 0.9523\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0265 - acc: 0.9662 - val_loss: 0.0415 - val_acc: 0.9429\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0252 - acc: 0.9682 - val_loss: 0.0513 - val_acc: 0.9281\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23601, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "13s - loss: 0.3040 - acc: 0.9019 - val_loss: 0.2360 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23601 to 0.18950, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.2094 - acc: 0.9168 - val_loss: 0.1895 - val_acc: 0.9335\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18950 to 0.16309, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1752 - acc: 0.9362 - val_loss: 0.1631 - val_acc: 0.9410\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16309 to 0.15577, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1599 - acc: 0.9411 - val_loss: 0.1558 - val_acc: 0.9437\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15577 to 0.14925, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1483 - acc: 0.9447 - val_loss: 0.1492 - val_acc: 0.9453\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14925 to 0.13759, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1400 - acc: 0.9483 - val_loss: 0.1376 - val_acc: 0.9495\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13759 to 0.13227, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1331 - acc: 0.9507 - val_loss: 0.1323 - val_acc: 0.9511\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13227 to 0.12876, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1270 - acc: 0.9534 - val_loss: 0.1288 - val_acc: 0.9516\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12876 to 0.12814, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1224 - acc: 0.9552 - val_loss: 0.1281 - val_acc: 0.9521\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12814 to 0.12338, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1181 - acc: 0.9564 - val_loss: 0.1234 - val_acc: 0.9539\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12338 to 0.12176, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1142 - acc: 0.9584 - val_loss: 0.1218 - val_acc: 0.9546\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "11s - loss: 0.1105 - acc: 0.9600 - val_loss: 0.1237 - val_acc: 0.9551\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.12176 to 0.12019, saving model to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "11s - loss: 0.1066 - acc: 0.9610 - val_loss: 0.1202 - val_acc: 0.9557\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "11s - loss: 0.1034 - acc: 0.9625 - val_loss: 0.1469 - val_acc: 0.9407\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "11s - loss: 0.1001 - acc: 0.9639 - val_loss: 0.1320 - val_acc: 0.9544\n",
      "Epoch 00014: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_102_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03351, saving model to ../models/atn_cnn_wiki_tox_v4_102_probs_model.h5\n",
      "12s - loss: 0.0269 - acc: 0.9648 - val_loss: 0.0335 - val_acc: 0.9562\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0258 - acc: 0.9670 - val_loss: 0.0336 - val_acc: 0.9559\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0247 - acc: 0.9686 - val_loss: 0.0368 - val_acc: 0.9553\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24168, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "13s - loss: 0.3097 - acc: 0.9028 - val_loss: 0.2417 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24168 to 0.21083, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.2206 - acc: 0.9096 - val_loss: 0.2108 - val_acc: 0.9278\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21083 to 0.18287, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1909 - acc: 0.9313 - val_loss: 0.1829 - val_acc: 0.9309\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18287 to 0.16837, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1728 - acc: 0.9376 - val_loss: 0.1684 - val_acc: 0.9411\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16837 to 0.15971, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1579 - acc: 0.9420 - val_loss: 0.1597 - val_acc: 0.9375\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15971 to 0.14858, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1473 - acc: 0.9459 - val_loss: 0.1486 - val_acc: 0.9424\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14858 to 0.13404, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1385 - acc: 0.9483 - val_loss: 0.1340 - val_acc: 0.9491\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13404 to 0.13175, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1306 - acc: 0.9515 - val_loss: 0.1317 - val_acc: 0.9498\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13175 to 0.12623, saving model to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "11s - loss: 0.1251 - acc: 0.9528 - val_loss: 0.1262 - val_acc: 0.9517\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "11s - loss: 0.1203 - acc: 0.9551 - val_loss: 0.1828 - val_acc: 0.9240\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "11s - loss: 0.1161 - acc: 0.9569 - val_loss: 0.1387 - val_acc: 0.9478\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_103_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03463, saving model to ../models/atn_cnn_wiki_tox_v4_103_probs_model.h5\n",
      "12s - loss: 0.0313 - acc: 0.9583 - val_loss: 0.0346 - val_acc: 0.9527\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0302 - acc: 0.9602 - val_loss: 0.0437 - val_acc: 0.9391\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03463 to 0.03425, saving model to ../models/atn_cnn_wiki_tox_v4_103_probs_model.h5\n",
      "11s - loss: 0.0289 - acc: 0.9626 - val_loss: 0.0343 - val_acc: 0.9552\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "11s - loss: 0.0278 - acc: 0.9645 - val_loss: 0.0345 - val_acc: 0.9536\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.0265 - acc: 0.9662 - val_loss: 0.0358 - val_acc: 0.9520\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23524, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "13s - loss: 0.3070 - acc: 0.9013 - val_loss: 0.2352 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23524 to 0.18524, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.2131 - acc: 0.9157 - val_loss: 0.1852 - val_acc: 0.9337\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002: val_loss improved from 0.18524 to 0.16163, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1755 - acc: 0.9361 - val_loss: 0.1616 - val_acc: 0.9409\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16163 to 0.15645, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1583 - acc: 0.9416 - val_loss: 0.1564 - val_acc: 0.9429\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15645 to 0.14141, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1464 - acc: 0.9458 - val_loss: 0.1414 - val_acc: 0.9466\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14141 to 0.14015, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1378 - acc: 0.9492 - val_loss: 0.1401 - val_acc: 0.9489\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14015 to 0.13171, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1315 - acc: 0.9516 - val_loss: 0.1317 - val_acc: 0.9494\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13171 to 0.12758, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1256 - acc: 0.9540 - val_loss: 0.1276 - val_acc: 0.9523\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12758 to 0.12560, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1214 - acc: 0.9558 - val_loss: 0.1256 - val_acc: 0.9527\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "11s - loss: 0.1170 - acc: 0.9572 - val_loss: 0.1307 - val_acc: 0.9528\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12560 to 0.12405, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1137 - acc: 0.9585 - val_loss: 0.1240 - val_acc: 0.9531\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12405 to 0.12100, saving model to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "11s - loss: 0.1100 - acc: 0.9600 - val_loss: 0.1210 - val_acc: 0.9549\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "11s - loss: 0.1070 - acc: 0.9609 - val_loss: 0.1242 - val_acc: 0.9557\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "11s - loss: 0.1036 - acc: 0.9626 - val_loss: 0.1229 - val_acc: 0.9537\n",
      "Epoch 00013: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_104_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03320, saving model to ../models/atn_cnn_wiki_tox_v4_104_probs_model.h5\n",
      "12s - loss: 0.0279 - acc: 0.9641 - val_loss: 0.0332 - val_acc: 0.9570\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0268 - acc: 0.9658 - val_loss: 0.0333 - val_acc: 0.9569\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0257 - acc: 0.9674 - val_loss: 0.0332 - val_acc: 0.9578\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24521, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "13s - loss: 0.3136 - acc: 0.9033 - val_loss: 0.2452 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24521 to 0.20281, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.2251 - acc: 0.9080 - val_loss: 0.2028 - val_acc: 0.9271\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20281 to 0.17971, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.1924 - acc: 0.9308 - val_loss: 0.1797 - val_acc: 0.9371\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17971 to 0.16022, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.1718 - acc: 0.9370 - val_loss: 0.1602 - val_acc: 0.9415\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16022 to 0.15024, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.1581 - acc: 0.9413 - val_loss: 0.1502 - val_acc: 0.9445\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "11s - loss: 0.1472 - acc: 0.9458 - val_loss: 0.1576 - val_acc: 0.9448\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.15024 to 0.14367, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.1391 - acc: 0.9484 - val_loss: 0.1437 - val_acc: 0.9482\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14367 to 0.13621, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.1329 - acc: 0.9506 - val_loss: 0.1362 - val_acc: 0.9502\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13621 to 0.12883, saving model to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "11s - loss: 0.1273 - acc: 0.9524 - val_loss: 0.1288 - val_acc: 0.9517\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "11s - loss: 0.1229 - acc: 0.9539 - val_loss: 0.1377 - val_acc: 0.9522\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "11s - loss: 0.1181 - acc: 0.9556 - val_loss: 0.1358 - val_acc: 0.9536\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_105_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03520, saving model to ../models/atn_cnn_wiki_tox_v4_105_probs_model.h5\n",
      "12s - loss: 0.0322 - acc: 0.9571 - val_loss: 0.0352 - val_acc: 0.9543\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0308 - acc: 0.9595 - val_loss: 0.0366 - val_acc: 0.9526\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0298 - acc: 0.9611 - val_loss: 0.0364 - val_acc: 0.9524\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.25343, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "13s - loss: 0.3154 - acc: 0.9033 - val_loss: 0.2534 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.25343 to 0.21481, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.2326 - acc: 0.9035 - val_loss: 0.2148 - val_acc: 0.9053\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21481 to 0.19635, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.2030 - acc: 0.9219 - val_loss: 0.1964 - val_acc: 0.9341\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.19635 to 0.18214, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1873 - acc: 0.9337 - val_loss: 0.1821 - val_acc: 0.9351\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.18214 to 0.17138, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1737 - acc: 0.9376 - val_loss: 0.1714 - val_acc: 0.9336\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.17138 to 0.15661, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1599 - acc: 0.9412 - val_loss: 0.1566 - val_acc: 0.9415\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.15661 to 0.14957, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1509 - acc: 0.9447 - val_loss: 0.1496 - val_acc: 0.9442\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14957 to 0.14354, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1429 - acc: 0.9473 - val_loss: 0.1435 - val_acc: 0.9465\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "11s - loss: 0.1363 - acc: 0.9494 - val_loss: 0.1824 - val_acc: 0.9373\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: val_loss improved from 0.14354 to 0.13993, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1304 - acc: 0.9517 - val_loss: 0.1399 - val_acc: 0.9498\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "11s - loss: 0.1252 - acc: 0.9537 - val_loss: 0.1552 - val_acc: 0.9476\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.13993 to 0.13845, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1205 - acc: 0.9554 - val_loss: 0.1384 - val_acc: 0.9517\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.13845 to 0.13558, saving model to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "11s - loss: 0.1164 - acc: 0.9574 - val_loss: 0.1356 - val_acc: 0.9487\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "11s - loss: 0.1127 - acc: 0.9587 - val_loss: 0.1384 - val_acc: 0.9500\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "11s - loss: 0.1087 - acc: 0.9598 - val_loss: 0.1631 - val_acc: 0.9384\n",
      "Epoch 00014: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_106_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03500, saving model to ../models/atn_cnn_wiki_tox_v4_106_probs_model.h5\n",
      "12s - loss: 0.0295 - acc: 0.9609 - val_loss: 0.0350 - val_acc: 0.9529\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0280 - acc: 0.9639 - val_loss: 0.0412 - val_acc: 0.9444\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0270 - acc: 0.9648 - val_loss: 0.0378 - val_acc: 0.9490\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24173, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "14s - loss: 0.3028 - acc: 0.9034 - val_loss: 0.2417 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24173 to 0.21303, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.2199 - acc: 0.9121 - val_loss: 0.2130 - val_acc: 0.9243\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21303 to 0.17693, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1894 - acc: 0.9322 - val_loss: 0.1769 - val_acc: 0.9368\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17693 to 0.16664, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1716 - acc: 0.9377 - val_loss: 0.1666 - val_acc: 0.9358\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16664 to 0.14814, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1577 - acc: 0.9420 - val_loss: 0.1481 - val_acc: 0.9439\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14814 to 0.14326, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1470 - acc: 0.9462 - val_loss: 0.1433 - val_acc: 0.9466\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14326 to 0.13999, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1389 - acc: 0.9481 - val_loss: 0.1400 - val_acc: 0.9484\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13999 to 0.13789, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1325 - acc: 0.9507 - val_loss: 0.1379 - val_acc: 0.9503\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13789 to 0.12813, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1266 - acc: 0.9535 - val_loss: 0.1281 - val_acc: 0.9509\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12813 to 0.12633, saving model to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "11s - loss: 0.1223 - acc: 0.9550 - val_loss: 0.1263 - val_acc: 0.9525\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "11s - loss: 0.1183 - acc: 0.9559 - val_loss: 0.1266 - val_acc: 0.9528\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "11s - loss: 0.1140 - acc: 0.9580 - val_loss: 0.1336 - val_acc: 0.9527\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_107_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.04038, saving model to ../models/atn_cnn_wiki_tox_v4_107_probs_model.h5\n",
      "12s - loss: 0.0309 - acc: 0.9590 - val_loss: 0.0404 - val_acc: 0.9462\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.04038 to 0.03680, saving model to ../models/atn_cnn_wiki_tox_v4_107_probs_model.h5\n",
      "11s - loss: 0.0297 - acc: 0.9606 - val_loss: 0.0368 - val_acc: 0.9501\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03680 to 0.03513, saving model to ../models/atn_cnn_wiki_tox_v4_107_probs_model.h5\n",
      "11s - loss: 0.0286 - acc: 0.9626 - val_loss: 0.0351 - val_acc: 0.9517\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03513 to 0.03472, saving model to ../models/atn_cnn_wiki_tox_v4_107_probs_model.h5\n",
      "11s - loss: 0.0274 - acc: 0.9645 - val_loss: 0.0347 - val_acc: 0.9533\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.0265 - acc: 0.9659 - val_loss: 0.0352 - val_acc: 0.9517\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.03472 to 0.03385, saving model to ../models/atn_cnn_wiki_tox_v4_107_probs_model.h5\n",
      "11s - loss: 0.0253 - acc: 0.9677 - val_loss: 0.0339 - val_acc: 0.9556\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "11s - loss: 0.0245 - acc: 0.9690 - val_loss: 0.0347 - val_acc: 0.9561\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "11s - loss: 0.0233 - acc: 0.9713 - val_loss: 0.0363 - val_acc: 0.9552\n",
      "Epoch 00007: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23281, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "14s - loss: 0.3016 - acc: 0.9020 - val_loss: 0.2328 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23281 to 0.18777, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "11s - loss: 0.2114 - acc: 0.9172 - val_loss: 0.1878 - val_acc: 0.9340\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18777 to 0.16814, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "11s - loss: 0.1794 - acc: 0.9365 - val_loss: 0.1681 - val_acc: 0.9406\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16814 to 0.16160, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "11s - loss: 0.1615 - acc: 0.9414 - val_loss: 0.1616 - val_acc: 0.9427\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16160 to 0.14300, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "11s - loss: 0.1491 - acc: 0.9455 - val_loss: 0.1430 - val_acc: 0.9453\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14300 to 0.13445, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "11s - loss: 0.1401 - acc: 0.9480 - val_loss: 0.1345 - val_acc: 0.9490\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13445 to 0.12931, saving model to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "11s - loss: 0.1320 - acc: 0.9510 - val_loss: 0.1293 - val_acc: 0.9510\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "11s - loss: 0.1267 - acc: 0.9536 - val_loss: 0.1326 - val_acc: 0.9495\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "11s - loss: 0.1214 - acc: 0.9556 - val_loss: 0.1403 - val_acc: 0.9459\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_108_model.h5\n",
      "Fitting probs model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03462, saving model to ../models/atn_cnn_wiki_tox_v4_108_probs_model.h5\n",
      "12s - loss: 0.0329 - acc: 0.9567 - val_loss: 0.0346 - val_acc: 0.9546\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0314 - acc: 0.9595 - val_loss: 0.0389 - val_acc: 0.9468\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03462 to 0.03425, saving model to ../models/atn_cnn_wiki_tox_v4_108_probs_model.h5\n",
      "11s - loss: 0.0305 - acc: 0.9611 - val_loss: 0.0342 - val_acc: 0.9549\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "11s - loss: 0.0292 - acc: 0.9632 - val_loss: 0.0385 - val_acc: 0.9471\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.0279 - acc: 0.9649 - val_loss: 0.0396 - val_acc: 0.9472\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23959, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "14s - loss: 0.3052 - acc: 0.9009 - val_loss: 0.2396 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23959 to 0.19995, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.2183 - acc: 0.9098 - val_loss: 0.2000 - val_acc: 0.9222\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19995 to 0.18057, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1897 - acc: 0.9316 - val_loss: 0.1806 - val_acc: 0.9376\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18057 to 0.16236, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1737 - acc: 0.9376 - val_loss: 0.1624 - val_acc: 0.9395\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16236 to 0.14869, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1567 - acc: 0.9426 - val_loss: 0.1487 - val_acc: 0.9432\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14869 to 0.14061, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1453 - acc: 0.9465 - val_loss: 0.1406 - val_acc: 0.9474\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "11s - loss: 0.1367 - acc: 0.9490 - val_loss: 0.1483 - val_acc: 0.9469\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14061 to 0.13764, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1295 - acc: 0.9517 - val_loss: 0.1376 - val_acc: 0.9495\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13764 to 0.12740, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1244 - acc: 0.9537 - val_loss: 0.1274 - val_acc: 0.9522\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "11s - loss: 0.1198 - acc: 0.9556 - val_loss: 0.1499 - val_acc: 0.9455\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12740 to 0.12541, saving model to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "11s - loss: 0.1156 - acc: 0.9564 - val_loss: 0.1254 - val_acc: 0.9519\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "11s - loss: 0.1115 - acc: 0.9585 - val_loss: 0.1724 - val_acc: 0.9384\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "11s - loss: 0.1079 - acc: 0.9596 - val_loss: 0.1263 - val_acc: 0.9533\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_wiki_tox_v4_109_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03451, saving model to ../models/atn_cnn_wiki_tox_v4_109_probs_model.h5\n",
      "12s - loss: 0.0293 - acc: 0.9609 - val_loss: 0.0345 - val_acc: 0.9538\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0280 - acc: 0.9633 - val_loss: 0.0438 - val_acc: 0.9385\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0268 - acc: 0.9657 - val_loss: 0.0363 - val_acc: 0.9510\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "model_names = ['atn_cnn_wiki_tox_v4_{}'.format(i) for i in xrange(100, 110)]\n",
    "for model_name in model_names:\n",
    "    MODEL_NAME = model_name\n",
    "    wiki_model = AttentionToxModel(hparams=hparams)\n",
    "    wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9599183439957785"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiased attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23235, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "15s - loss: 0.2965 - acc: 0.9065 - val_loss: 0.2324 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23235 to 0.19087, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.2108 - acc: 0.9180 - val_loss: 0.1909 - val_acc: 0.9336\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19087 to 0.17190, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1826 - acc: 0.9359 - val_loss: 0.1719 - val_acc: 0.9372\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17190 to 0.15262, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1639 - acc: 0.9410 - val_loss: 0.1526 - val_acc: 0.9437\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15262 to 0.14286, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1507 - acc: 0.9449 - val_loss: 0.1429 - val_acc: 0.9467\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14286 to 0.14081, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1413 - acc: 0.9485 - val_loss: 0.1408 - val_acc: 0.9482\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14081 to 0.12945, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1334 - acc: 0.9513 - val_loss: 0.1295 - val_acc: 0.9509\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12945 to 0.12616, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1270 - acc: 0.9530 - val_loss: 0.1262 - val_acc: 0.9523\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12616 to 0.12420, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1217 - acc: 0.9553 - val_loss: 0.1242 - val_acc: 0.9535\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "12s - loss: 0.1175 - acc: 0.9570 - val_loss: 0.1372 - val_acc: 0.9513\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12420 to 0.12353, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1130 - acc: 0.9580 - val_loss: 0.1235 - val_acc: 0.9543\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12353 to 0.11933, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1092 - acc: 0.9593 - val_loss: 0.1193 - val_acc: 0.9561\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "12s - loss: 0.1058 - acc: 0.9609 - val_loss: 0.1217 - val_acc: 0.9551\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss improved from 0.11933 to 0.11906, saving model to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "12s - loss: 0.1021 - acc: 0.9618 - val_loss: 0.1191 - val_acc: 0.9564\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "12s - loss: 0.0987 - acc: 0.9633 - val_loss: 0.1355 - val_acc: 0.9501\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "12s - loss: 0.0952 - acc: 0.9643 - val_loss: 0.1205 - val_acc: 0.9562\n",
      "Epoch 00015: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_100_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03260, saving model to ../models/atn_cnn_debias_tox_v4_100_probs_model.h5\n",
      "14s - loss: 0.0257 - acc: 0.9662 - val_loss: 0.0326 - val_acc: 0.9576\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "11s - loss: 0.0245 - acc: 0.9681 - val_loss: 0.0331 - val_acc: 0.9570\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0232 - acc: 0.9705 - val_loss: 0.0446 - val_acc: 0.9397\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.25366, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "15s - loss: 0.3133 - acc: 0.9063 - val_loss: 0.2537 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.25366 to 0.21543, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.2343 - acc: 0.9068 - val_loss: 0.2154 - val_acc: 0.9079\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21543 to 0.19200, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.2052 - acc: 0.9207 - val_loss: 0.1920 - val_acc: 0.9334\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.19200 to 0.18127, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.1890 - acc: 0.9341 - val_loss: 0.1813 - val_acc: 0.9369\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.18127 to 0.17155, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.1777 - acc: 0.9383 - val_loss: 0.1716 - val_acc: 0.9365\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.17155 to 0.17104, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.1674 - acc: 0.9408 - val_loss: 0.1710 - val_acc: 0.9351\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.17104 to 0.15733, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.1580 - acc: 0.9438 - val_loss: 0.1573 - val_acc: 0.9405\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.15733 to 0.14736, saving model to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "12s - loss: 0.1500 - acc: 0.9459 - val_loss: 0.1474 - val_acc: 0.9464\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "12s - loss: 0.1427 - acc: 0.9483 - val_loss: 0.1531 - val_acc: 0.9423\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "12s - loss: 0.1371 - acc: 0.9506 - val_loss: 0.1853 - val_acc: 0.9350\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_101_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03899, saving model to ../models/atn_cnn_debias_tox_v4_101_probs_model.h5\n",
      "14s - loss: 0.0366 - acc: 0.9525 - val_loss: 0.0390 - val_acc: 0.9487\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03899 to 0.03758, saving model to ../models/atn_cnn_debias_tox_v4_101_probs_model.h5\n",
      "12s - loss: 0.0353 - acc: 0.9544 - val_loss: 0.0376 - val_acc: 0.9508\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "11s - loss: 0.0338 - acc: 0.9566 - val_loss: 0.0437 - val_acc: 0.9432\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03758 to 0.03689, saving model to ../models/atn_cnn_debias_tox_v4_101_probs_model.h5\n",
      "12s - loss: 0.0327 - acc: 0.9581 - val_loss: 0.0369 - val_acc: 0.9518\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "11s - loss: 0.0316 - acc: 0.9597 - val_loss: 0.0578 - val_acc: 0.9234\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "11s - loss: 0.0303 - acc: 0.9617 - val_loss: 0.0372 - val_acc: 0.9493\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23285, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "15s - loss: 0.2949 - acc: 0.9053 - val_loss: 0.2329 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23285 to 0.18998, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.2135 - acc: 0.9183 - val_loss: 0.1900 - val_acc: 0.9332\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18998 to 0.16787, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1821 - acc: 0.9361 - val_loss: 0.1679 - val_acc: 0.9403\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16787 to 0.15722, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1631 - acc: 0.9416 - val_loss: 0.1572 - val_acc: 0.9424\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15722 to 0.14233, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1509 - acc: 0.9447 - val_loss: 0.1423 - val_acc: 0.9472\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14233 to 0.13561, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1411 - acc: 0.9484 - val_loss: 0.1356 - val_acc: 0.9491\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13561 to 0.12940, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1332 - acc: 0.9505 - val_loss: 0.1294 - val_acc: 0.9514\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12940 to 0.12539, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1274 - acc: 0.9533 - val_loss: 0.1254 - val_acc: 0.9539\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "12s - loss: 0.1215 - acc: 0.9554 - val_loss: 0.1265 - val_acc: 0.9536\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12539 to 0.12059, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1175 - acc: 0.9576 - val_loss: 0.1206 - val_acc: 0.9559\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12059 to 0.11973, saving model to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "12s - loss: 0.1138 - acc: 0.9586 - val_loss: 0.1197 - val_acc: 0.9558\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "12s - loss: 0.1102 - acc: 0.9596 - val_loss: 0.1221 - val_acc: 0.9548\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "12s - loss: 0.1065 - acc: 0.9612 - val_loss: 0.1500 - val_acc: 0.9461\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_102_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03454, saving model to ../models/atn_cnn_debias_tox_v4_102_probs_model.h5\n",
      "14s - loss: 0.0288 - acc: 0.9624 - val_loss: 0.0345 - val_acc: 0.9538\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03454 to 0.03316, saving model to ../models/atn_cnn_debias_tox_v4_102_probs_model.h5\n",
      "12s - loss: 0.0275 - acc: 0.9646 - val_loss: 0.0332 - val_acc: 0.9555\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "12s - loss: 0.0264 - acc: 0.9661 - val_loss: 0.0351 - val_acc: 0.9557\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "11s - loss: 0.0254 - acc: 0.9678 - val_loss: 0.0333 - val_acc: 0.9559\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22878, saving model to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "15s - loss: 0.2952 - acc: 0.9063 - val_loss: 0.2288 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22878 to 0.18940, saving model to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "12s - loss: 0.2098 - acc: 0.9159 - val_loss: 0.1894 - val_acc: 0.9335\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18940 to 0.16539, saving model to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "12s - loss: 0.1816 - acc: 0.9356 - val_loss: 0.1654 - val_acc: 0.9408\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16539 to 0.15141, saving model to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "12s - loss: 0.1623 - acc: 0.9415 - val_loss: 0.1514 - val_acc: 0.9444\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15141 to 0.14373, saving model to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "12s - loss: 0.1492 - acc: 0.9454 - val_loss: 0.1437 - val_acc: 0.9469\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14373 to 0.13442, saving model to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "12s - loss: 0.1398 - acc: 0.9485 - val_loss: 0.1344 - val_acc: 0.9497\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "12s - loss: 0.1321 - acc: 0.9516 - val_loss: 0.1501 - val_acc: 0.9498\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "12s - loss: 0.1269 - acc: 0.9536 - val_loss: 0.1474 - val_acc: 0.9489\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_103_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03667, saving model to ../models/atn_cnn_debias_tox_v4_103_probs_model.h5\n",
      "15s - loss: 0.0341 - acc: 0.9553 - val_loss: 0.0367 - val_acc: 0.9533\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03667 to 0.03462, saving model to ../models/atn_cnn_debias_tox_v4_103_probs_model.h5\n",
      "12s - loss: 0.0326 - acc: 0.9575 - val_loss: 0.0346 - val_acc: 0.9545\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03462 to 0.03394, saving model to ../models/atn_cnn_debias_tox_v4_103_probs_model.h5\n",
      "12s - loss: 0.0313 - acc: 0.9593 - val_loss: 0.0339 - val_acc: 0.9557\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03394 to 0.03318, saving model to ../models/atn_cnn_debias_tox_v4_103_probs_model.h5\n",
      "12s - loss: 0.0301 - acc: 0.9610 - val_loss: 0.0332 - val_acc: 0.9568\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "12s - loss: 0.0290 - acc: 0.9631 - val_loss: 0.0414 - val_acc: 0.9445\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "12s - loss: 0.0280 - acc: 0.9648 - val_loss: 0.0352 - val_acc: 0.9530\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22915, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "15s - loss: 0.3006 - acc: 0.9040 - val_loss: 0.2292 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22915 to 0.18232, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.2085 - acc: 0.9191 - val_loss: 0.1823 - val_acc: 0.9357\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18232 to 0.16171, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.1741 - acc: 0.9374 - val_loss: 0.1617 - val_acc: 0.9415\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16171 to 0.15275, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.1587 - acc: 0.9425 - val_loss: 0.1528 - val_acc: 0.9448\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15275 to 0.14191, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.1481 - acc: 0.9463 - val_loss: 0.1419 - val_acc: 0.9471\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14191 to 0.13991, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.1403 - acc: 0.9493 - val_loss: 0.1399 - val_acc: 0.9487\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: val_loss improved from 0.13991 to 0.13271, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.1331 - acc: 0.9518 - val_loss: 0.1327 - val_acc: 0.9510\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13271 to 0.12807, saving model to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "12s - loss: 0.1265 - acc: 0.9542 - val_loss: 0.1281 - val_acc: 0.9535\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "12s - loss: 0.1216 - acc: 0.9558 - val_loss: 0.1290 - val_acc: 0.9529\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "12s - loss: 0.1171 - acc: 0.9578 - val_loss: 0.1365 - val_acc: 0.9477\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_104_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03399, saving model to ../models/atn_cnn_debias_tox_v4_104_probs_model.h5\n",
      "15s - loss: 0.0313 - acc: 0.9594 - val_loss: 0.0340 - val_acc: 0.9563\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "12s - loss: 0.0301 - acc: 0.9608 - val_loss: 0.0468 - val_acc: 0.9356\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "12s - loss: 0.0289 - acc: 0.9633 - val_loss: 0.0407 - val_acc: 0.9447\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23369, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "15s - loss: 0.2980 - acc: 0.9066 - val_loss: 0.2337 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23369 to 0.18937, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.2140 - acc: 0.9182 - val_loss: 0.1894 - val_acc: 0.9337\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18937 to 0.16709, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1798 - acc: 0.9357 - val_loss: 0.1671 - val_acc: 0.9400\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16709 to 0.15416, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1635 - acc: 0.9405 - val_loss: 0.1542 - val_acc: 0.9440\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15416 to 0.14902, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1524 - acc: 0.9444 - val_loss: 0.1490 - val_acc: 0.9465\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14902 to 0.14067, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1437 - acc: 0.9482 - val_loss: 0.1407 - val_acc: 0.9474\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14067 to 0.13587, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1358 - acc: 0.9509 - val_loss: 0.1359 - val_acc: 0.9484\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13587 to 0.13242, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1296 - acc: 0.9530 - val_loss: 0.1324 - val_acc: 0.9508\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13242 to 0.12499, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1247 - acc: 0.9548 - val_loss: 0.1250 - val_acc: 0.9539\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12499 to 0.12274, saving model to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "12s - loss: 0.1200 - acc: 0.9567 - val_loss: 0.1227 - val_acc: 0.9547\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "12s - loss: 0.1157 - acc: 0.9583 - val_loss: 0.1415 - val_acc: 0.9446\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "12s - loss: 0.1126 - acc: 0.9598 - val_loss: 0.1391 - val_acc: 0.9457\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_105_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03450, saving model to ../models/atn_cnn_debias_tox_v4_105_probs_model.h5\n",
      "15s - loss: 0.0302 - acc: 0.9610 - val_loss: 0.0345 - val_acc: 0.9542\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03450 to 0.03395, saving model to ../models/atn_cnn_debias_tox_v4_105_probs_model.h5\n",
      "12s - loss: 0.0292 - acc: 0.9625 - val_loss: 0.0340 - val_acc: 0.9559\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03395 to 0.03394, saving model to ../models/atn_cnn_debias_tox_v4_105_probs_model.h5\n",
      "12s - loss: 0.0282 - acc: 0.9639 - val_loss: 0.0339 - val_acc: 0.9553\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03394 to 0.03379, saving model to ../models/atn_cnn_debias_tox_v4_105_probs_model.h5\n",
      "12s - loss: 0.0272 - acc: 0.9658 - val_loss: 0.0338 - val_acc: 0.9555\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "12s - loss: 0.0260 - acc: 0.9675 - val_loss: 0.0352 - val_acc: 0.9534\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "12s - loss: 0.0250 - acc: 0.9689 - val_loss: 0.0350 - val_acc: 0.9544\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23591, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "16s - loss: 0.3008 - acc: 0.9065 - val_loss: 0.2359 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23591 to 0.19456, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.2159 - acc: 0.9093 - val_loss: 0.1946 - val_acc: 0.9242\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19456 to 0.17705, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1885 - acc: 0.9317 - val_loss: 0.1771 - val_acc: 0.9396\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17705 to 0.16037, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1727 - acc: 0.9394 - val_loss: 0.1604 - val_acc: 0.9416\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16037 to 0.14750, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1566 - acc: 0.9430 - val_loss: 0.1475 - val_acc: 0.9452\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14750 to 0.14728, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1457 - acc: 0.9468 - val_loss: 0.1473 - val_acc: 0.9469\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14728 to 0.14724, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1376 - acc: 0.9497 - val_loss: 0.1472 - val_acc: 0.9465\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14724 to 0.13191, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1317 - acc: 0.9523 - val_loss: 0.1319 - val_acc: 0.9506\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13191 to 0.13103, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1255 - acc: 0.9538 - val_loss: 0.1310 - val_acc: 0.9516\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.13103 to 0.12708, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1212 - acc: 0.9550 - val_loss: 0.1271 - val_acc: 0.9533\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "12s - loss: 0.1172 - acc: 0.9567 - val_loss: 0.1331 - val_acc: 0.9505\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12708 to 0.12207, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1132 - acc: 0.9584 - val_loss: 0.1221 - val_acc: 0.9536\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: val_loss improved from 0.12207 to 0.12143, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1096 - acc: 0.9595 - val_loss: 0.1214 - val_acc: 0.9549\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss improved from 0.12143 to 0.12067, saving model to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "12s - loss: 0.1059 - acc: 0.9609 - val_loss: 0.1207 - val_acc: 0.9552\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "12s - loss: 0.1026 - acc: 0.9621 - val_loss: 0.1353 - val_acc: 0.9457\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "12s - loss: 0.0993 - acc: 0.9638 - val_loss: 0.1238 - val_acc: 0.9559\n",
      "Epoch 00015: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_106_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03754, saving model to ../models/atn_cnn_debias_tox_v4_106_probs_model.h5\n",
      "15s - loss: 0.0265 - acc: 0.9651 - val_loss: 0.0375 - val_acc: 0.9490\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03754 to 0.03384, saving model to ../models/atn_cnn_debias_tox_v4_106_probs_model.h5\n",
      "12s - loss: 0.0253 - acc: 0.9667 - val_loss: 0.0338 - val_acc: 0.9548\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "12s - loss: 0.0241 - acc: 0.9686 - val_loss: 0.0344 - val_acc: 0.9559\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "12s - loss: 0.0232 - acc: 0.9702 - val_loss: 0.0348 - val_acc: 0.9542\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24044, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "16s - loss: 0.3023 - acc: 0.9065 - val_loss: 0.2404 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24044 to 0.20410, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.2197 - acc: 0.9133 - val_loss: 0.2041 - val_acc: 0.9286\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20410 to 0.17560, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.1893 - acc: 0.9332 - val_loss: 0.1756 - val_acc: 0.9373\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17560 to 0.16998, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.1715 - acc: 0.9384 - val_loss: 0.1700 - val_acc: 0.9350\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16998 to 0.16086, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.1571 - acc: 0.9427 - val_loss: 0.1609 - val_acc: 0.9372\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.16086 to 0.14186, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.1469 - acc: 0.9459 - val_loss: 0.1419 - val_acc: 0.9459\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14186 to 0.13440, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.1390 - acc: 0.9489 - val_loss: 0.1344 - val_acc: 0.9499\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "12s - loss: 0.1317 - acc: 0.9516 - val_loss: 0.1460 - val_acc: 0.9451\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13440 to 0.12754, saving model to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "12s - loss: 0.1258 - acc: 0.9538 - val_loss: 0.1275 - val_acc: 0.9529\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "12s - loss: 0.1212 - acc: 0.9554 - val_loss: 0.1292 - val_acc: 0.9502\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "12s - loss: 0.1171 - acc: 0.9572 - val_loss: 0.1316 - val_acc: 0.9502\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_107_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03664, saving model to ../models/atn_cnn_debias_tox_v4_107_probs_model.h5\n",
      "15s - loss: 0.0315 - acc: 0.9582 - val_loss: 0.0366 - val_acc: 0.9506\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03664 to 0.03476, saving model to ../models/atn_cnn_debias_tox_v4_107_probs_model.h5\n",
      "12s - loss: 0.0301 - acc: 0.9607 - val_loss: 0.0348 - val_acc: 0.9531\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03476 to 0.03415, saving model to ../models/atn_cnn_debias_tox_v4_107_probs_model.h5\n",
      "12s - loss: 0.0291 - acc: 0.9625 - val_loss: 0.0341 - val_acc: 0.9550\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "12s - loss: 0.0279 - acc: 0.9641 - val_loss: 0.0372 - val_acc: 0.9492\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "12s - loss: 0.0266 - acc: 0.9666 - val_loss: 0.0370 - val_acc: 0.9498\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23611, saving model to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "16s - loss: 0.2979 - acc: 0.9054 - val_loss: 0.2361 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23611 to 0.18952, saving model to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "12s - loss: 0.2139 - acc: 0.9171 - val_loss: 0.1895 - val_acc: 0.9331\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18952 to 0.16534, saving model to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "12s - loss: 0.1799 - acc: 0.9358 - val_loss: 0.1653 - val_acc: 0.9402\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16534 to 0.15337, saving model to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "12s - loss: 0.1615 - acc: 0.9415 - val_loss: 0.1534 - val_acc: 0.9440\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15337 to 0.14231, saving model to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "12s - loss: 0.1493 - acc: 0.9455 - val_loss: 0.1423 - val_acc: 0.9475\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "12s - loss: 0.1391 - acc: 0.9490 - val_loss: 0.1452 - val_acc: 0.9444\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14231 to 0.12759, saving model to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "12s - loss: 0.1321 - acc: 0.9522 - val_loss: 0.1276 - val_acc: 0.9519\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "12s - loss: 0.1261 - acc: 0.9541 - val_loss: 0.1314 - val_acc: 0.9517\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "12s - loss: 0.1210 - acc: 0.9562 - val_loss: 0.1287 - val_acc: 0.9520\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_108_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03434, saving model to ../models/atn_cnn_debias_tox_v4_108_probs_model.h5\n",
      "15s - loss: 0.0325 - acc: 0.9573 - val_loss: 0.0343 - val_acc: 0.9552\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "12s - loss: 0.0313 - acc: 0.9595 - val_loss: 0.0349 - val_acc: 0.9551\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03434 to 0.03360, saving model to ../models/atn_cnn_debias_tox_v4_108_probs_model.h5\n",
      "12s - loss: 0.0300 - acc: 0.9612 - val_loss: 0.0336 - val_acc: 0.9563\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03360 to 0.03324, saving model to ../models/atn_cnn_debias_tox_v4_108_probs_model.h5\n",
      "12s - loss: 0.0288 - acc: 0.9633 - val_loss: 0.0332 - val_acc: 0.9566\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "12s - loss: 0.0278 - acc: 0.9651 - val_loss: 0.0339 - val_acc: 0.9551\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: val_loss did not improve\n",
      "12s - loss: 0.0268 - acc: 0.9667 - val_loss: 0.0341 - val_acc: 0.9563\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23670, saving model to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "16s - loss: 0.3046 - acc: 0.9066 - val_loss: 0.2367 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23670 to 0.20402, saving model to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "12s - loss: 0.2206 - acc: 0.9068 - val_loss: 0.2040 - val_acc: 0.9098\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20402 to 0.18059, saving model to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "12s - loss: 0.1927 - acc: 0.9211 - val_loss: 0.1806 - val_acc: 0.9323\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18059 to 0.17143, saving model to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "12s - loss: 0.1795 - acc: 0.9354 - val_loss: 0.1714 - val_acc: 0.9404\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.17143 to 0.16787, saving model to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "12s - loss: 0.1672 - acc: 0.9407 - val_loss: 0.1679 - val_acc: 0.9417\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "12s - loss: 0.1540 - acc: 0.9438 - val_loss: 0.1700 - val_acc: 0.9407\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.16787 to 0.14318, saving model to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "12s - loss: 0.1437 - acc: 0.9471 - val_loss: 0.1432 - val_acc: 0.9478\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "12s - loss: 0.1365 - acc: 0.9491 - val_loss: 0.1545 - val_acc: 0.9479\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "12s - loss: 0.1307 - acc: 0.9516 - val_loss: 0.1614 - val_acc: 0.9438\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/atn_cnn_debias_tox_v4_109_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03773, saving model to ../models/atn_cnn_debias_tox_v4_109_probs_model.h5\n",
      "15s - loss: 0.0349 - acc: 0.9539 - val_loss: 0.0377 - val_acc: 0.9501\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03773 to 0.03615, saving model to ../models/atn_cnn_debias_tox_v4_109_probs_model.h5\n",
      "12s - loss: 0.0333 - acc: 0.9571 - val_loss: 0.0361 - val_acc: 0.9527\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03615 to 0.03581, saving model to ../models/atn_cnn_debias_tox_v4_109_probs_model.h5\n",
      "12s - loss: 0.0318 - acc: 0.9587 - val_loss: 0.0358 - val_acc: 0.9532\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "12s - loss: 0.0307 - acc: 0.9610 - val_loss: 0.0406 - val_acc: 0.9465\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "12s - loss: 0.0294 - acc: 0.9627 - val_loss: 0.0362 - val_acc: 0.9518\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "model_names = ['atn_cnn_debias_tox_v4_{}'.format(i) for i in xrange(100, 110)]\n",
    "for model_name in model_names:\n",
    "    MODEL_NAME = model_name\n",
    "    debias_model = AttentionToxModel(hparams=hparams)\n",
    "    debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9476482738664272"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.score_auc(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
