{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Toxicity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a model to detect toxicity in online comments. It uses a CNN architecture for text classification trained on the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and pre-trained GloVe embeddings which can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "This model is a modification of [example code](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) found in the [Keras Github repository](https://github.com/fchollet/keras) and released under an [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE). For further details of this license, find it [online](https://github.com/fchollet/keras/blob/master/LICENSE) or in this repository in the file KERAS_LICENSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "(TODO: nthain) - Move to README\n",
    "\n",
    "Prior to running the notebook, you must:\n",
    "\n",
    "* Download the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)\n",
    "* Download pre-trained [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "* (optional) To skip the training step, you will need to download a model and tokenizer file. We are looking into the appropriate means for distributing these (sometimes large) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model_tool import ToxModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17138, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "9s - loss: 0.2394 - acc: 0.9180 - val_loss: 0.1714 - val_acc: 0.9363\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17138 to 0.16345, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1649 - acc: 0.9404 - val_loss: 0.1634 - val_acc: 0.9386\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.16345 to 0.14203, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1470 - acc: 0.9466 - val_loss: 0.1420 - val_acc: 0.9483\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14203 to 0.13586, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1350 - acc: 0.9501 - val_loss: 0.1359 - val_acc: 0.9509\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13586 to 0.12960, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1261 - acc: 0.9531 - val_loss: 0.1296 - val_acc: 0.9516\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12960 to 0.12748, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1184 - acc: 0.9560 - val_loss: 0.1275 - val_acc: 0.9540\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12748 to 0.12281, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1131 - acc: 0.9582 - val_loss: 0.1228 - val_acc: 0.9547\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1075 - acc: 0.9607 - val_loss: 0.1235 - val_acc: 0.9545\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12281 to 0.12210, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1026 - acc: 0.9624 - val_loss: 0.1221 - val_acc: 0.9560\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0972 - acc: 0.9646 - val_loss: 0.1244 - val_acc: 0.9563\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "8s - loss: 0.0923 - acc: 0.9659 - val_loss: 0.1244 - val_acc: 0.9566\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_debias_random_tox_v3'\n",
    "debias_random_model = ToxModel()\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96207693660952776"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "debias_random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17343, saving model to ../models/cnn_wiki_tox_v3_model.h5\n",
      "8s - loss: 0.2423 - acc: 0.9159 - val_loss: 0.1734 - val_acc: 0.9363\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17343 to 0.15115, saving model to ../models/cnn_wiki_tox_v3_model.h5\n",
      "8s - loss: 0.1670 - acc: 0.9385 - val_loss: 0.1512 - val_acc: 0.9437\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.1479 - acc: 0.9448 - val_loss: 0.1550 - val_acc: 0.9464\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.15115 to 0.13282, saving model to ../models/cnn_wiki_tox_v3_model.h5\n",
      "8s - loss: 0.1361 - acc: 0.9498 - val_loss: 0.1328 - val_acc: 0.9494\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1274 - acc: 0.9531 - val_loss: 0.1357 - val_acc: 0.9506\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1204 - acc: 0.9552 - val_loss: 0.1341 - val_acc: 0.9481\n",
      "Epoch 00005: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_wiki_tox_v3'\n",
    "wiki_model = ToxModel()\n",
    "wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95431884126081146"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16940, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.2318 - acc: 0.9199 - val_loss: 0.1694 - val_acc: 0.9393\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16940 to 0.14565, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.1608 - acc: 0.9415 - val_loss: 0.1456 - val_acc: 0.9472\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14565 to 0.14244, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.1432 - acc: 0.9479 - val_loss: 0.1424 - val_acc: 0.9470\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14244 to 0.13064, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.1330 - acc: 0.9513 - val_loss: 0.1306 - val_acc: 0.9515\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13064 to 0.12709, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.1252 - acc: 0.9539 - val_loss: 0.1271 - val_acc: 0.9528\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1184 - acc: 0.9568 - val_loss: 0.1327 - val_acc: 0.9511\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12709 to 0.12277, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.1124 - acc: 0.9586 - val_loss: 0.1228 - val_acc: 0.9550\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12277 to 0.12173, saving model to ../models/cnn_debias_tox_v3_model.h5\n",
      "8s - loss: 0.1065 - acc: 0.9605 - val_loss: 0.1217 - val_acc: 0.9560\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1004 - acc: 0.9631 - val_loss: 0.1317 - val_acc: 0.9553\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0949 - acc: 0.9652 - val_loss: 0.1256 - val_acc: 0.9544\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_debias_tox_v3'\n",
    "debias_model = ToxModel()\n",
    "debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ToxModel instance has no attribute 'prep_data_and_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c2e330991cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdebias_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdebias_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_data_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebias_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebias_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: ToxModel instance has no attribute 'prep_data_and_score'"
     ]
    }
   ],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.prep_data_and_score(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
