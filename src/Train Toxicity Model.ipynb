{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Toxicity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a model to detect toxicity in online comments. It uses a CNN architecture for text classification trained on the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and pre-trained GloVe embeddings which can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "This model is a modification of [example code](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) found in the [Keras Github repository](https://github.com/fchollet/keras) and released under an [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE). For further details of this license, find it [online](https://github.com/fchollet/keras/blob/master/LICENSE) or in this repository in the file KERAS_LICENSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "(TODO: nthain) - Move to README\n",
    "\n",
    "Prior to running the notebook, you must:\n",
    "\n",
    "* Download the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)\n",
    "* Download pre-trained [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "* (optional) To skip the training step, you will need to download a model and tokenizer file. We are looking into the appropriate means for distributing these (sometimes large) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model_tool import ToxModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16887, saving model to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "9s - loss: 0.2342 - acc: 0.9189 - val_loss: 0.1689 - val_acc: 0.9387\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16887 to 0.14691, saving model to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "8s - loss: 0.1607 - acc: 0.9409 - val_loss: 0.1469 - val_acc: 0.9461\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14691 to 0.13582, saving model to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "8s - loss: 0.1418 - acc: 0.9478 - val_loss: 0.1358 - val_acc: 0.9504\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13582 to 0.13041, saving model to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "8s - loss: 0.1305 - acc: 0.9521 - val_loss: 0.1304 - val_acc: 0.9528\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13041 to 0.12383, saving model to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "8s - loss: 0.1221 - acc: 0.9547 - val_loss: 0.1238 - val_acc: 0.9543\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12383 to 0.12183, saving model to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "8s - loss: 0.1155 - acc: 0.9575 - val_loss: 0.1218 - val_acc: 0.9552\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1094 - acc: 0.9595 - val_loss: 0.1317 - val_acc: 0.9542\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1042 - acc: 0.9614 - val_loss: 0.1346 - val_acc: 0.9481\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17080, saving model to ../models/cnn_debias_random_tox_v3_101_model.h5\n",
      "9s - loss: 0.2335 - acc: 0.9189 - val_loss: 0.1708 - val_acc: 0.9377\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17080 to 0.15214, saving model to ../models/cnn_debias_random_tox_v3_101_model.h5\n",
      "8s - loss: 0.1626 - acc: 0.9406 - val_loss: 0.1521 - val_acc: 0.9431\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15214 to 0.14563, saving model to ../models/cnn_debias_random_tox_v3_101_model.h5\n",
      "8s - loss: 0.1439 - acc: 0.9468 - val_loss: 0.1456 - val_acc: 0.9444\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14563 to 0.13457, saving model to ../models/cnn_debias_random_tox_v3_101_model.h5\n",
      "8s - loss: 0.1315 - acc: 0.9508 - val_loss: 0.1346 - val_acc: 0.9487\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1228 - acc: 0.9545 - val_loss: 0.1477 - val_acc: 0.9505\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13457 to 0.12751, saving model to ../models/cnn_debias_random_tox_v3_101_model.h5\n",
      "8s - loss: 0.1158 - acc: 0.9571 - val_loss: 0.1275 - val_acc: 0.9541\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1100 - acc: 0.9594 - val_loss: 0.1350 - val_acc: 0.9478\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1041 - acc: 0.9614 - val_loss: 0.1384 - val_acc: 0.9461\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16682, saving model to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "8s - loss: 0.2299 - acc: 0.9211 - val_loss: 0.1668 - val_acc: 0.9400\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16682 to 0.14370, saving model to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "8s - loss: 0.1597 - acc: 0.9415 - val_loss: 0.1437 - val_acc: 0.9467\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14370 to 0.13411, saving model to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "8s - loss: 0.1423 - acc: 0.9478 - val_loss: 0.1341 - val_acc: 0.9501\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13411 to 0.12911, saving model to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "8s - loss: 0.1304 - acc: 0.9522 - val_loss: 0.1291 - val_acc: 0.9510\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.12911 to 0.12473, saving model to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "8s - loss: 0.1225 - acc: 0.9550 - val_loss: 0.1247 - val_acc: 0.9534\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1161 - acc: 0.9579 - val_loss: 0.1252 - val_acc: 0.9546\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12473 to 0.12143, saving model to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "8s - loss: 0.1094 - acc: 0.9604 - val_loss: 0.1214 - val_acc: 0.9550\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1040 - acc: 0.9620 - val_loss: 0.1217 - val_acc: 0.9557\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.0984 - acc: 0.9639 - val_loss: 0.1230 - val_acc: 0.9562\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16613, saving model to ../models/cnn_debias_random_tox_v3_103_model.h5\n",
      "9s - loss: 0.2300 - acc: 0.9196 - val_loss: 0.1661 - val_acc: 0.9396\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16613 to 0.14770, saving model to ../models/cnn_debias_random_tox_v3_103_model.h5\n",
      "8s - loss: 0.1608 - acc: 0.9422 - val_loss: 0.1477 - val_acc: 0.9464\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14770 to 0.13575, saving model to ../models/cnn_debias_random_tox_v3_103_model.h5\n",
      "8s - loss: 0.1429 - acc: 0.9473 - val_loss: 0.1357 - val_acc: 0.9493\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.1332 - acc: 0.9510 - val_loss: 0.1459 - val_acc: 0.9443\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13575 to 0.12731, saving model to ../models/cnn_debias_random_tox_v3_103_model.h5\n",
      "8s - loss: 0.1239 - acc: 0.9550 - val_loss: 0.1273 - val_acc: 0.9536\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1170 - acc: 0.9566 - val_loss: 0.1343 - val_acc: 0.9486\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12731 to 0.12471, saving model to ../models/cnn_debias_random_tox_v3_103_model.h5\n",
      "8s - loss: 0.1116 - acc: 0.9592 - val_loss: 0.1247 - val_acc: 0.9560\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1058 - acc: 0.9612 - val_loss: 0.1452 - val_acc: 0.9432\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1007 - acc: 0.9630 - val_loss: 0.1291 - val_acc: 0.9560\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18119, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "9s - loss: 0.2347 - acc: 0.9184 - val_loss: 0.1812 - val_acc: 0.9379\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18119 to 0.14666, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "8s - loss: 0.1622 - acc: 0.9408 - val_loss: 0.1467 - val_acc: 0.9452\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14666 to 0.13878, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "8s - loss: 0.1440 - acc: 0.9473 - val_loss: 0.1388 - val_acc: 0.9477\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.1324 - acc: 0.9519 - val_loss: 0.1394 - val_acc: 0.9504\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13878 to 0.13048, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "8s - loss: 0.1241 - acc: 0.9543 - val_loss: 0.1305 - val_acc: 0.9502\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13048 to 0.12419, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "8s - loss: 0.1170 - acc: 0.9571 - val_loss: 0.1242 - val_acc: 0.9545\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12419 to 0.12297, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "8s - loss: 0.1111 - acc: 0.9593 - val_loss: 0.1230 - val_acc: 0.9552\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1052 - acc: 0.9616 - val_loss: 0.1233 - val_acc: 0.9557\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12297 to 0.12164, saving model to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "8s - loss: 0.0998 - acc: 0.9634 - val_loss: 0.1216 - val_acc: 0.9565\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0945 - acc: 0.9658 - val_loss: 0.1240 - val_acc: 0.9559\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "8s - loss: 0.0894 - acc: 0.9674 - val_loss: 0.1404 - val_acc: 0.9562\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16912, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "9s - loss: 0.2335 - acc: 0.9200 - val_loss: 0.1691 - val_acc: 0.9393\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16912 to 0.14886, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1623 - acc: 0.9409 - val_loss: 0.1489 - val_acc: 0.9451\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14886 to 0.14071, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1446 - acc: 0.9471 - val_loss: 0.1407 - val_acc: 0.9466\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14071 to 0.13354, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1328 - acc: 0.9510 - val_loss: 0.1335 - val_acc: 0.9516\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13354 to 0.12906, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1240 - acc: 0.9544 - val_loss: 0.1291 - val_acc: 0.9505\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12906 to 0.12326, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1165 - acc: 0.9569 - val_loss: 0.1233 - val_acc: 0.9544\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12326 to 0.12223, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1102 - acc: 0.9596 - val_loss: 0.1222 - val_acc: 0.9546\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12223 to 0.12094, saving model to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "8s - loss: 0.1047 - acc: 0.9616 - val_loss: 0.1209 - val_acc: 0.9549\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.0991 - acc: 0.9639 - val_loss: 0.1390 - val_acc: 0.9463\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0940 - acc: 0.9656 - val_loss: 0.1243 - val_acc: 0.9566\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16872, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "9s - loss: 0.2328 - acc: 0.9194 - val_loss: 0.1687 - val_acc: 0.9388\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16872 to 0.14896, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "8s - loss: 0.1616 - acc: 0.9406 - val_loss: 0.1490 - val_acc: 0.9458\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14896 to 0.13557, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "8s - loss: 0.1426 - acc: 0.9474 - val_loss: 0.1356 - val_acc: 0.9498\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13557 to 0.13027, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "8s - loss: 0.1320 - acc: 0.9512 - val_loss: 0.1303 - val_acc: 0.9525\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13027 to 0.12840, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "8s - loss: 0.1230 - acc: 0.9544 - val_loss: 0.1284 - val_acc: 0.9537\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12840 to 0.12421, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "8s - loss: 0.1166 - acc: 0.9568 - val_loss: 0.1242 - val_acc: 0.9547\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12421 to 0.12393, saving model to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "8s - loss: 0.1110 - acc: 0.9591 - val_loss: 0.1239 - val_acc: 0.9553\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1049 - acc: 0.9616 - val_loss: 0.1239 - val_acc: 0.9536\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.0999 - acc: 0.9631 - val_loss: 0.1251 - val_acc: 0.9538\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17391, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "9s - loss: 0.2413 - acc: 0.9161 - val_loss: 0.1739 - val_acc: 0.9367\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17391 to 0.14993, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1666 - acc: 0.9390 - val_loss: 0.1499 - val_acc: 0.9452\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14993 to 0.13633, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1457 - acc: 0.9463 - val_loss: 0.1363 - val_acc: 0.9490\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13633 to 0.13114, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1333 - acc: 0.9509 - val_loss: 0.1311 - val_acc: 0.9511\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13114 to 0.13000, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1243 - acc: 0.9537 - val_loss: 0.1300 - val_acc: 0.9532\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13000 to 0.12536, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1182 - acc: 0.9563 - val_loss: 0.1254 - val_acc: 0.9537\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12536 to 0.12411, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1114 - acc: 0.9590 - val_loss: 0.1241 - val_acc: 0.9535\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12411 to 0.12206, saving model to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "8s - loss: 0.1061 - acc: 0.9606 - val_loss: 0.1221 - val_acc: 0.9568\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1011 - acc: 0.9628 - val_loss: 0.1234 - val_acc: 0.9569\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0954 - acc: 0.9650 - val_loss: 0.1347 - val_acc: 0.9564\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17337, saving model to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "9s - loss: 0.2359 - acc: 0.9179 - val_loss: 0.1734 - val_acc: 0.9366\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17337 to 0.15126, saving model to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "8s - loss: 0.1650 - acc: 0.9396 - val_loss: 0.1513 - val_acc: 0.9451\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.1458 - acc: 0.9462 - val_loss: 0.1547 - val_acc: 0.9412\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.15126 to 0.12961, saving model to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "8s - loss: 0.1331 - acc: 0.9508 - val_loss: 0.1296 - val_acc: 0.9518\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.12961 to 0.12545, saving model to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "9s - loss: 0.1244 - acc: 0.9538 - val_loss: 0.1254 - val_acc: 0.9532\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12545 to 0.12514, saving model to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "9s - loss: 0.1172 - acc: 0.9566 - val_loss: 0.1251 - val_acc: 0.9528\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1112 - acc: 0.9591 - val_loss: 0.1475 - val_acc: 0.9432\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12514 to 0.11960, saving model to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "9s - loss: 0.1051 - acc: 0.9609 - val_loss: 0.1196 - val_acc: 0.9565\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.0998 - acc: 0.9634 - val_loss: 0.1245 - val_acc: 0.9573\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0946 - acc: 0.9648 - val_loss: 0.1274 - val_acc: 0.9578\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17332, saving model to ../models/cnn_debias_random_tox_v3_109_model.h5\n",
      "9s - loss: 0.2324 - acc: 0.9186 - val_loss: 0.1733 - val_acc: 0.9374\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17332 to 0.14890, saving model to ../models/cnn_debias_random_tox_v3_109_model.h5\n",
      "8s - loss: 0.1597 - acc: 0.9418 - val_loss: 0.1489 - val_acc: 0.9447\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14890 to 0.13481, saving model to ../models/cnn_debias_random_tox_v3_109_model.h5\n",
      "9s - loss: 0.1424 - acc: 0.9481 - val_loss: 0.1348 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13481 to 0.13398, saving model to ../models/cnn_debias_random_tox_v3_109_model.h5\n",
      "9s - loss: 0.1311 - acc: 0.9519 - val_loss: 0.1340 - val_acc: 0.9523\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1225 - acc: 0.9551 - val_loss: 0.1801 - val_acc: 0.9286\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1159 - acc: 0.9578 - val_loss: 0.1405 - val_acc: 0.9453\n",
      "Epoch 00005: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(100, 110):\n",
    "    MODEL_NAME = 'cnn_debias_random_tox_v3_{}'.format(i)\n",
    "    debias_random_model = ToxModel()\n",
    "    debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95278138776342269"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "debias_random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18188, saving model to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "9s - loss: 0.2384 - acc: 0.9154 - val_loss: 0.1819 - val_acc: 0.9354\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18188 to 0.15620, saving model to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "8s - loss: 0.1705 - acc: 0.9373 - val_loss: 0.1562 - val_acc: 0.9416\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15620 to 0.14465, saving model to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "8s - loss: 0.1515 - acc: 0.9437 - val_loss: 0.1446 - val_acc: 0.9467\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.1392 - acc: 0.9482 - val_loss: 0.1455 - val_acc: 0.9485\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.14465 to 0.13893, saving model to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "8s - loss: 0.1301 - acc: 0.9518 - val_loss: 0.1389 - val_acc: 0.9506\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13893 to 0.13068, saving model to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "8s - loss: 0.1225 - acc: 0.9552 - val_loss: 0.1307 - val_acc: 0.9523\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1159 - acc: 0.9571 - val_loss: 0.1311 - val_acc: 0.9508\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13068 to 0.12764, saving model to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "8s - loss: 0.1098 - acc: 0.9593 - val_loss: 0.1276 - val_acc: 0.9542\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1039 - acc: 0.9611 - val_loss: 0.1500 - val_acc: 0.9527\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0993 - acc: 0.9638 - val_loss: 0.1287 - val_acc: 0.9547\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17658, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "9s - loss: 0.2411 - acc: 0.9147 - val_loss: 0.1766 - val_acc: 0.9354\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17658 to 0.15127, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "8s - loss: 0.1677 - acc: 0.9383 - val_loss: 0.1513 - val_acc: 0.9438\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15127 to 0.14099, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "8s - loss: 0.1480 - acc: 0.9455 - val_loss: 0.1410 - val_acc: 0.9471\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14099 to 0.13403, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "8s - loss: 0.1367 - acc: 0.9496 - val_loss: 0.1340 - val_acc: 0.9495\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13403 to 0.13247, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "8s - loss: 0.1278 - acc: 0.9527 - val_loss: 0.1325 - val_acc: 0.9504\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1212 - acc: 0.9549 - val_loss: 0.1327 - val_acc: 0.9522\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13247 to 0.12587, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "8s - loss: 0.1147 - acc: 0.9577 - val_loss: 0.1259 - val_acc: 0.9526\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12587 to 0.12549, saving model to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "8s - loss: 0.1091 - acc: 0.9597 - val_loss: 0.1255 - val_acc: 0.9539\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1035 - acc: 0.9617 - val_loss: 0.1325 - val_acc: 0.9547\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0983 - acc: 0.9641 - val_loss: 0.1275 - val_acc: 0.9538\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17642, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "10s - loss: 0.2387 - acc: 0.9168 - val_loss: 0.1764 - val_acc: 0.9354\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17642 to 0.14985, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1677 - acc: 0.9382 - val_loss: 0.1499 - val_acc: 0.9443\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14985 to 0.13879, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1482 - acc: 0.9452 - val_loss: 0.1388 - val_acc: 0.9481\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13879 to 0.13385, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1362 - acc: 0.9497 - val_loss: 0.1338 - val_acc: 0.9491\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13385 to 0.12916, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1275 - acc: 0.9528 - val_loss: 0.1292 - val_acc: 0.9519\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12916 to 0.12828, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1204 - acc: 0.9551 - val_loss: 0.1283 - val_acc: 0.9539\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12828 to 0.12779, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1141 - acc: 0.9577 - val_loss: 0.1278 - val_acc: 0.9512\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12779 to 0.12402, saving model to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "8s - loss: 0.1081 - acc: 0.9597 - val_loss: 0.1240 - val_acc: 0.9533\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1028 - acc: 0.9619 - val_loss: 0.1369 - val_acc: 0.9547\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0969 - acc: 0.9646 - val_loss: 0.1249 - val_acc: 0.9531\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18092, saving model to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "9s - loss: 0.2399 - acc: 0.9161 - val_loss: 0.1809 - val_acc: 0.9346\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18092 to 0.14978, saving model to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "8s - loss: 0.1661 - acc: 0.9388 - val_loss: 0.1498 - val_acc: 0.9451\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14978 to 0.13900, saving model to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "8s - loss: 0.1471 - acc: 0.9458 - val_loss: 0.1390 - val_acc: 0.9482\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: val_loss improved from 0.13900 to 0.13266, saving model to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "8s - loss: 0.1350 - acc: 0.9501 - val_loss: 0.1327 - val_acc: 0.9506\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1260 - acc: 0.9530 - val_loss: 0.1342 - val_acc: 0.9522\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13266 to 0.12538, saving model to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "8s - loss: 0.1188 - acc: 0.9565 - val_loss: 0.1254 - val_acc: 0.9531\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12538 to 0.12468, saving model to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "8s - loss: 0.1131 - acc: 0.9580 - val_loss: 0.1247 - val_acc: 0.9537\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1069 - acc: 0.9606 - val_loss: 0.1276 - val_acc: 0.9547\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1012 - acc: 0.9625 - val_loss: 0.1312 - val_acc: 0.9508\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17617, saving model to ../models/cnn_wiki_tox_v3_104_model.h5\n",
      "10s - loss: 0.2427 - acc: 0.9153 - val_loss: 0.1762 - val_acc: 0.9357\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.1695 - acc: 0.9382 - val_loss: 0.1777 - val_acc: 0.9334\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.17617 to 0.15035, saving model to ../models/cnn_wiki_tox_v3_104_model.h5\n",
      "8s - loss: 0.1509 - acc: 0.9441 - val_loss: 0.1504 - val_acc: 0.9435\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.15035 to 0.13635, saving model to ../models/cnn_wiki_tox_v3_104_model.h5\n",
      "8s - loss: 0.1385 - acc: 0.9487 - val_loss: 0.1363 - val_acc: 0.9495\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13635 to 0.13191, saving model to ../models/cnn_wiki_tox_v3_104_model.h5\n",
      "8s - loss: 0.1290 - acc: 0.9522 - val_loss: 0.1319 - val_acc: 0.9504\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13191 to 0.12906, saving model to ../models/cnn_wiki_tox_v3_104_model.h5\n",
      "8s - loss: 0.1218 - acc: 0.9556 - val_loss: 0.1291 - val_acc: 0.9522\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1156 - acc: 0.9570 - val_loss: 0.1325 - val_acc: 0.9526\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1098 - acc: 0.9593 - val_loss: 0.1441 - val_acc: 0.9447\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.19287, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "10s - loss: 0.2479 - acc: 0.9132 - val_loss: 0.1929 - val_acc: 0.9307\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.19287 to 0.16241, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "8s - loss: 0.1684 - acc: 0.9377 - val_loss: 0.1624 - val_acc: 0.9422\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.16241 to 0.14040, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "8s - loss: 0.1487 - acc: 0.9444 - val_loss: 0.1404 - val_acc: 0.9482\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14040 to 0.13862, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "8s - loss: 0.1369 - acc: 0.9497 - val_loss: 0.1386 - val_acc: 0.9498\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13862 to 0.13054, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "8s - loss: 0.1280 - acc: 0.9529 - val_loss: 0.1305 - val_acc: 0.9518\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13054 to 0.12879, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "8s - loss: 0.1211 - acc: 0.9554 - val_loss: 0.1288 - val_acc: 0.9530\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12879 to 0.12409, saving model to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "8s - loss: 0.1150 - acc: 0.9579 - val_loss: 0.1241 - val_acc: 0.9542\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1092 - acc: 0.9593 - val_loss: 0.1334 - val_acc: 0.9545\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1039 - acc: 0.9615 - val_loss: 0.1305 - val_acc: 0.9501\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17881, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "10s - loss: 0.2425 - acc: 0.9146 - val_loss: 0.1788 - val_acc: 0.9358\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17881 to 0.15301, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "8s - loss: 0.1675 - acc: 0.9383 - val_loss: 0.1530 - val_acc: 0.9434\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15301 to 0.14778, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "8s - loss: 0.1497 - acc: 0.9447 - val_loss: 0.1478 - val_acc: 0.9466\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14778 to 0.13749, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "8s - loss: 0.1386 - acc: 0.9490 - val_loss: 0.1375 - val_acc: 0.9492\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1303 - acc: 0.9518 - val_loss: 0.1384 - val_acc: 0.9478\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13749 to 0.13167, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "8s - loss: 0.1230 - acc: 0.9547 - val_loss: 0.1317 - val_acc: 0.9514\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13167 to 0.12866, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "8s - loss: 0.1163 - acc: 0.9570 - val_loss: 0.1287 - val_acc: 0.9527\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1099 - acc: 0.9597 - val_loss: 0.1329 - val_acc: 0.9539\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12866 to 0.12794, saving model to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "8s - loss: 0.1049 - acc: 0.9617 - val_loss: 0.1279 - val_acc: 0.9540\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0995 - acc: 0.9636 - val_loss: 0.1396 - val_acc: 0.9538\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "8s - loss: 0.0943 - acc: 0.9657 - val_loss: 0.1339 - val_acc: 0.9547\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18286, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "10s - loss: 0.2392 - acc: 0.9162 - val_loss: 0.1829 - val_acc: 0.9341\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18286 to 0.15677, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "8s - loss: 0.1661 - acc: 0.9387 - val_loss: 0.1568 - val_acc: 0.9417\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15677 to 0.15115, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "8s - loss: 0.1476 - acc: 0.9456 - val_loss: 0.1512 - val_acc: 0.9432\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.15115 to 0.14173, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "8s - loss: 0.1355 - acc: 0.9497 - val_loss: 0.1417 - val_acc: 0.9493\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.14173 to 0.13145, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "8s - loss: 0.1275 - acc: 0.9529 - val_loss: 0.1315 - val_acc: 0.9501\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13145 to 0.12791, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "8s - loss: 0.1199 - acc: 0.9557 - val_loss: 0.1279 - val_acc: 0.9527\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12791 to 0.12498, saving model to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "8s - loss: 0.1137 - acc: 0.9575 - val_loss: 0.1250 - val_acc: 0.9535\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1075 - acc: 0.9600 - val_loss: 0.1260 - val_acc: 0.9520\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1019 - acc: 0.9619 - val_loss: 0.1284 - val_acc: 0.9548\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18139, saving model to ../models/cnn_wiki_tox_v3_108_model.h5\n",
      "10s - loss: 0.2387 - acc: 0.9160 - val_loss: 0.1814 - val_acc: 0.9350\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18139 to 0.16144, saving model to ../models/cnn_wiki_tox_v3_108_model.h5\n",
      "8s - loss: 0.1669 - acc: 0.9388 - val_loss: 0.1614 - val_acc: 0.9432\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.16144 to 0.14771, saving model to ../models/cnn_wiki_tox_v3_108_model.h5\n",
      "8s - loss: 0.1481 - acc: 0.9452 - val_loss: 0.1477 - val_acc: 0.9469\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14771 to 0.13462, saving model to ../models/cnn_wiki_tox_v3_108_model.h5\n",
      "8s - loss: 0.1354 - acc: 0.9502 - val_loss: 0.1346 - val_acc: 0.9494\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1263 - acc: 0.9532 - val_loss: 0.1374 - val_acc: 0.9516\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1199 - acc: 0.9562 - val_loss: 0.1366 - val_acc: 0.9477\n",
      "Epoch 00005: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.18299, saving model to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "10s - loss: 0.2367 - acc: 0.9168 - val_loss: 0.1830 - val_acc: 0.9348\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.18299 to 0.15096, saving model to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "8s - loss: 0.1676 - acc: 0.9377 - val_loss: 0.1510 - val_acc: 0.9435\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15096 to 0.14067, saving model to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "8s - loss: 0.1482 - acc: 0.9445 - val_loss: 0.1407 - val_acc: 0.9475\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14067 to 0.13396, saving model to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "8s - loss: 0.1359 - acc: 0.9491 - val_loss: 0.1340 - val_acc: 0.9501\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13396 to 0.13151, saving model to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "8s - loss: 0.1270 - acc: 0.9525 - val_loss: 0.1315 - val_acc: 0.9503\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13151 to 0.12625, saving model to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "8s - loss: 0.1197 - acc: 0.9558 - val_loss: 0.1262 - val_acc: 0.9535\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1133 - acc: 0.9578 - val_loss: 0.1271 - val_acc: 0.9541\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1074 - acc: 0.9609 - val_loss: 0.1417 - val_acc: 0.9457\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_wiki_tox_v3_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(100, 110):\n",
    "    MODEL_NAME = 'cnn_wiki_tox_v3_{}'.format(i)\n",
    "    wiki_model = ToxModel()\n",
    "    wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95696629963337654"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17132, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "10s - loss: 0.2267 - acc: 0.9213 - val_loss: 0.1713 - val_acc: 0.9376\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17132 to 0.15116, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "9s - loss: 0.1631 - acc: 0.9409 - val_loss: 0.1512 - val_acc: 0.9446\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15116 to 0.14667, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "9s - loss: 0.1448 - acc: 0.9470 - val_loss: 0.1467 - val_acc: 0.9442\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14667 to 0.13084, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "9s - loss: 0.1341 - acc: 0.9510 - val_loss: 0.1308 - val_acc: 0.9514\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13084 to 0.12629, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "9s - loss: 0.1255 - acc: 0.9542 - val_loss: 0.1263 - val_acc: 0.9528\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12629 to 0.12491, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "9s - loss: 0.1178 - acc: 0.9572 - val_loss: 0.1249 - val_acc: 0.9543\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12491 to 0.12181, saving model to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "9s - loss: 0.1116 - acc: 0.9594 - val_loss: 0.1218 - val_acc: 0.9550\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1053 - acc: 0.9617 - val_loss: 0.1229 - val_acc: 0.9561\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.0993 - acc: 0.9642 - val_loss: 0.1252 - val_acc: 0.9558\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_100_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17400, saving model to ../models/cnn_debias_tox_v3_101_model.h5\n",
      "10s - loss: 0.2373 - acc: 0.9172 - val_loss: 0.1740 - val_acc: 0.9368\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17400 to 0.15703, saving model to ../models/cnn_debias_tox_v3_101_model.h5\n",
      "9s - loss: 0.1650 - acc: 0.9397 - val_loss: 0.1570 - val_acc: 0.9444\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15703 to 0.13890, saving model to ../models/cnn_debias_tox_v3_101_model.h5\n",
      "9s - loss: 0.1459 - acc: 0.9464 - val_loss: 0.1389 - val_acc: 0.9489\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13890 to 0.13266, saving model to ../models/cnn_debias_tox_v3_101_model.h5\n",
      "9s - loss: 0.1343 - acc: 0.9508 - val_loss: 0.1327 - val_acc: 0.9499\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.1261 - acc: 0.9536 - val_loss: 0.1358 - val_acc: 0.9488\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.1183 - acc: 0.9566 - val_loss: 0.1371 - val_acc: 0.9527\n",
      "Epoch 00005: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_101_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16766, saving model to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "11s - loss: 0.2336 - acc: 0.9185 - val_loss: 0.1677 - val_acc: 0.9391\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16766 to 0.14685, saving model to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "9s - loss: 0.1618 - acc: 0.9416 - val_loss: 0.1469 - val_acc: 0.9465\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14685 to 0.13494, saving model to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "9s - loss: 0.1426 - acc: 0.9483 - val_loss: 0.1349 - val_acc: 0.9507\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13494 to 0.12979, saving model to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "9s - loss: 0.1319 - acc: 0.9512 - val_loss: 0.1298 - val_acc: 0.9523\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.1232 - acc: 0.9551 - val_loss: 0.1365 - val_acc: 0.9518\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12979 to 0.12418, saving model to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "9s - loss: 0.1170 - acc: 0.9571 - val_loss: 0.1242 - val_acc: 0.9545\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12418 to 0.12365, saving model to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "9s - loss: 0.1117 - acc: 0.9593 - val_loss: 0.1237 - val_acc: 0.9558\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1053 - acc: 0.9616 - val_loss: 0.1264 - val_acc: 0.9536\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.0998 - acc: 0.9636 - val_loss: 0.1471 - val_acc: 0.9435\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_102_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17215, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "11s - loss: 0.2405 - acc: 0.9175 - val_loss: 0.1722 - val_acc: 0.9379\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17215 to 0.14863, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "9s - loss: 0.1646 - acc: 0.9401 - val_loss: 0.1486 - val_acc: 0.9460\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14863 to 0.14209, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "9s - loss: 0.1471 - acc: 0.9462 - val_loss: 0.1421 - val_acc: 0.9481\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.14209 to 0.13287, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "9s - loss: 0.1361 - acc: 0.9499 - val_loss: 0.1329 - val_acc: 0.9508\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13287 to 0.12955, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "9s - loss: 0.1268 - acc: 0.9530 - val_loss: 0.1295 - val_acc: 0.9516\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12955 to 0.12563, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "9s - loss: 0.1199 - acc: 0.9559 - val_loss: 0.1256 - val_acc: 0.9542\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12563 to 0.12355, saving model to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "9s - loss: 0.1141 - acc: 0.9580 - val_loss: 0.1235 - val_acc: 0.9547\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1076 - acc: 0.9605 - val_loss: 0.1285 - val_acc: 0.9523\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1020 - acc: 0.9629 - val_loss: 0.1244 - val_acc: 0.9545\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_103_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17438, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "11s - loss: 0.2409 - acc: 0.9163 - val_loss: 0.1744 - val_acc: 0.9371\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17438 to 0.15323, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "8s - loss: 0.1653 - acc: 0.9392 - val_loss: 0.1532 - val_acc: 0.9436\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15323 to 0.13851, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "9s - loss: 0.1460 - acc: 0.9465 - val_loss: 0.1385 - val_acc: 0.9488\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13851 to 0.13363, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "8s - loss: 0.1343 - acc: 0.9506 - val_loss: 0.1336 - val_acc: 0.9510\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13363 to 0.13332, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "9s - loss: 0.1255 - acc: 0.9544 - val_loss: 0.1333 - val_acc: 0.9524\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13332 to 0.12942, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "9s - loss: 0.1186 - acc: 0.9561 - val_loss: 0.1294 - val_acc: 0.9539\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12942 to 0.12422, saving model to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "9s - loss: 0.1130 - acc: 0.9585 - val_loss: 0.1242 - val_acc: 0.9545\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1072 - acc: 0.9609 - val_loss: 0.1390 - val_acc: 0.9472\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1022 - acc: 0.9629 - val_loss: 0.1351 - val_acc: 0.9493\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_104_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17381, saving model to ../models/cnn_debias_tox_v3_105_model.h5\n",
      "11s - loss: 0.2455 - acc: 0.9124 - val_loss: 0.1738 - val_acc: 0.9369\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17381 to 0.15422, saving model to ../models/cnn_debias_tox_v3_105_model.h5\n",
      "9s - loss: 0.1665 - acc: 0.9395 - val_loss: 0.1542 - val_acc: 0.9445\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.15422 to 0.13896, saving model to ../models/cnn_debias_tox_v3_105_model.h5\n",
      "9s - loss: 0.1464 - acc: 0.9461 - val_loss: 0.1390 - val_acc: 0.9479\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13896 to 0.13194, saving model to ../models/cnn_debias_tox_v3_105_model.h5\n",
      "9s - loss: 0.1350 - acc: 0.9505 - val_loss: 0.1319 - val_acc: 0.9511\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.1270 - acc: 0.9534 - val_loss: 0.1347 - val_acc: 0.9522\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13194 to 0.12434, saving model to ../models/cnn_debias_tox_v3_105_model.h5\n",
      "9s - loss: 0.1202 - acc: 0.9558 - val_loss: 0.1243 - val_acc: 0.9535\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.1143 - acc: 0.9583 - val_loss: 0.1310 - val_acc: 0.9497\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1082 - acc: 0.9603 - val_loss: 0.1267 - val_acc: 0.9556\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_105_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16826, saving model to ../models/cnn_debias_tox_v3_106_model.h5\n",
      "11s - loss: 0.2296 - acc: 0.9205 - val_loss: 0.1683 - val_acc: 0.9392\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16826 to 0.14619, saving model to ../models/cnn_debias_tox_v3_106_model.h5\n",
      "9s - loss: 0.1618 - acc: 0.9410 - val_loss: 0.1462 - val_acc: 0.9459\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14619 to 0.13845, saving model to ../models/cnn_debias_tox_v3_106_model.h5\n",
      "9s - loss: 0.1444 - acc: 0.9466 - val_loss: 0.1384 - val_acc: 0.9499\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13845 to 0.13107, saving model to ../models/cnn_debias_tox_v3_106_model.h5\n",
      "9s - loss: 0.1331 - acc: 0.9512 - val_loss: 0.1311 - val_acc: 0.9519\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13107 to 0.12512, saving model to ../models/cnn_debias_tox_v3_106_model.h5\n",
      "9s - loss: 0.1245 - acc: 0.9542 - val_loss: 0.1251 - val_acc: 0.9533\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.1178 - acc: 0.9568 - val_loss: 0.1441 - val_acc: 0.9455\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1122 - acc: 0.9591 - val_loss: 0.1259 - val_acc: 0.9527\n",
      "Epoch 00006: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_106_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17212, saving model to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "11s - loss: 0.2288 - acc: 0.9208 - val_loss: 0.1721 - val_acc: 0.9384\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17212 to 0.14953, saving model to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "9s - loss: 0.1614 - acc: 0.9410 - val_loss: 0.1495 - val_acc: 0.9463\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14953 to 0.13589, saving model to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "9s - loss: 0.1433 - acc: 0.9480 - val_loss: 0.1359 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13589 to 0.13228, saving model to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "9s - loss: 0.1319 - acc: 0.9514 - val_loss: 0.1323 - val_acc: 0.9517\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13228 to 0.13046, saving model to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "9s - loss: 0.1233 - acc: 0.9547 - val_loss: 0.1305 - val_acc: 0.9531\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.1167 - acc: 0.9569 - val_loss: 0.1313 - val_acc: 0.9500\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13046 to 0.12439, saving model to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "8s - loss: 0.1108 - acc: 0.9597 - val_loss: 0.1244 - val_acc: 0.9545\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1047 - acc: 0.9612 - val_loss: 0.1289 - val_acc: 0.9545\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.0995 - acc: 0.9633 - val_loss: 0.1321 - val_acc: 0.9551\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_107_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17738, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "11s - loss: 0.2399 - acc: 0.9176 - val_loss: 0.1774 - val_acc: 0.9363\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17738 to 0.14597, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1628 - acc: 0.9414 - val_loss: 0.1460 - val_acc: 0.9461\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14597 to 0.13688, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1440 - acc: 0.9477 - val_loss: 0.1369 - val_acc: 0.9504\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13688 to 0.13316, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1324 - acc: 0.9523 - val_loss: 0.1332 - val_acc: 0.9514\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13316 to 0.12832, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1251 - acc: 0.9549 - val_loss: 0.1283 - val_acc: 0.9537\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12832 to 0.12359, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1182 - acc: 0.9570 - val_loss: 0.1236 - val_acc: 0.9539\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12359 to 0.12347, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1122 - acc: 0.9591 - val_loss: 0.1235 - val_acc: 0.9542\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12347 to 0.12262, saving model to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "9s - loss: 0.1063 - acc: 0.9617 - val_loss: 0.1226 - val_acc: 0.9559\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1013 - acc: 0.9627 - val_loss: 0.1237 - val_acc: 0.9546\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.0964 - acc: 0.9648 - val_loss: 0.1228 - val_acc: 0.9556\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_108_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.16852, saving model to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "11s - loss: 0.2321 - acc: 0.9191 - val_loss: 0.1685 - val_acc: 0.9394\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.16852 to 0.14724, saving model to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "9s - loss: 0.1630 - acc: 0.9409 - val_loss: 0.1472 - val_acc: 0.9455\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14724 to 0.13693, saving model to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "9s - loss: 0.1448 - acc: 0.9479 - val_loss: 0.1369 - val_acc: 0.9502\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.1339 - acc: 0.9510 - val_loss: 0.1395 - val_acc: 0.9512\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.13693 to 0.13007, saving model to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "9s - loss: 0.1257 - acc: 0.9544 - val_loss: 0.1301 - val_acc: 0.9517\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13007 to 0.12700, saving model to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "9s - loss: 0.1186 - acc: 0.9564 - val_loss: 0.1270 - val_acc: 0.9540\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12700 to 0.12415, saving model to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "9s - loss: 0.1125 - acc: 0.9582 - val_loss: 0.1241 - val_acc: 0.9550\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1072 - acc: 0.9604 - val_loss: 0.1257 - val_acc: 0.9540\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1016 - acc: 0.9625 - val_loss: 0.1259 - val_acc: 0.9562\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_tox_v3_109_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(100, 110):\n",
    "    MODEL_NAME = 'cnn_debias_tox_v3_{}'.format(i)\n",
    "    debias_model = ToxModel()\n",
    "    debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ToxModel instance has no attribute 'prep_data_and_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c2e330991cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdebias_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdebias_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_data_and_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebias_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebias_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: ToxModel instance has no attribute 'prep_data_and_score'"
     ]
    }
   ],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.prep_data_and_score(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
