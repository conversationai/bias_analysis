{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Toxicity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model_tool import AttentionToxModel\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23060, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "9s - loss: 0.2954 - acc: 0.9067 - val_loss: 0.2306 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23060 to 0.18672, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "8s - loss: 0.2112 - acc: 0.9177 - val_loss: 0.1867 - val_acc: 0.9344\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18672 to 0.16536, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "8s - loss: 0.1776 - acc: 0.9363 - val_loss: 0.1654 - val_acc: 0.9387\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16536 to 0.15113, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "9s - loss: 0.1602 - acc: 0.9408 - val_loss: 0.1511 - val_acc: 0.9424\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15113 to 0.14338, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "8s - loss: 0.1491 - acc: 0.9444 - val_loss: 0.1434 - val_acc: 0.9462\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14338 to 0.13495, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "8s - loss: 0.1398 - acc: 0.9481 - val_loss: 0.1350 - val_acc: 0.9479\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13495 to 0.13476, saving model to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "8s - loss: 0.1321 - acc: 0.9506 - val_loss: 0.1348 - val_acc: 0.9470\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1261 - acc: 0.9536 - val_loss: 0.1404 - val_acc: 0.9453\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1212 - acc: 0.9555 - val_loss: 0.1390 - val_acc: 0.9432\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_100_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03566, saving model to ../models/cnn_attention_random_tox_v2_100_probs_model.h5\n",
      "8s - loss: 0.0328 - acc: 0.9569 - val_loss: 0.0357 - val_acc: 0.9524\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0314 - acc: 0.9590 - val_loss: 0.0375 - val_acc: 0.9494\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03566 to 0.03370, saving model to ../models/cnn_attention_random_tox_v2_100_probs_model.h5\n",
      "8s - loss: 0.0304 - acc: 0.9606 - val_loss: 0.0337 - val_acc: 0.9556\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03370 to 0.03324, saving model to ../models/cnn_attention_random_tox_v2_100_probs_model.h5\n",
      "8s - loss: 0.0293 - acc: 0.9623 - val_loss: 0.0332 - val_acc: 0.9569\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.0282 - acc: 0.9643 - val_loss: 0.0505 - val_acc: 0.9277\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0269 - acc: 0.9663 - val_loss: 0.0340 - val_acc: 0.9546\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23265, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "9s - loss: 0.2966 - acc: 0.9063 - val_loss: 0.2326 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23265 to 0.19067, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "8s - loss: 0.2118 - acc: 0.9153 - val_loss: 0.1907 - val_acc: 0.9313\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19067 to 0.16721, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "8s - loss: 0.1821 - acc: 0.9354 - val_loss: 0.1672 - val_acc: 0.9402\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16721 to 0.15413, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "8s - loss: 0.1637 - acc: 0.9409 - val_loss: 0.1541 - val_acc: 0.9433\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15413 to 0.14624, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "9s - loss: 0.1513 - acc: 0.9445 - val_loss: 0.1462 - val_acc: 0.9455\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14624 to 0.13690, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "9s - loss: 0.1410 - acc: 0.9479 - val_loss: 0.1369 - val_acc: 0.9478\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1330 - acc: 0.9508 - val_loss: 0.1405 - val_acc: 0.9477\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13690 to 0.12679, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "8s - loss: 0.1261 - acc: 0.9529 - val_loss: 0.1268 - val_acc: 0.9522\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12679 to 0.12488, saving model to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "9s - loss: 0.1210 - acc: 0.9552 - val_loss: 0.1249 - val_acc: 0.9525\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1158 - acc: 0.9570 - val_loss: 0.1275 - val_acc: 0.9539\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1121 - acc: 0.9583 - val_loss: 0.1353 - val_acc: 0.9481\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_101_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03362, saving model to ../models/cnn_attention_random_tox_v2_101_probs_model.h5\n",
      "8s - loss: 0.0302 - acc: 0.9606 - val_loss: 0.0336 - val_acc: 0.9554\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0290 - acc: 0.9628 - val_loss: 0.0358 - val_acc: 0.9523\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0277 - acc: 0.9648 - val_loss: 0.0338 - val_acc: 0.9552\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23567, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.2949 - acc: 0.9067 - val_loss: 0.2357 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23567 to 0.19331, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "8s - loss: 0.2125 - acc: 0.9144 - val_loss: 0.1933 - val_acc: 0.9312\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19331 to 0.17238, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.1853 - acc: 0.9340 - val_loss: 0.1724 - val_acc: 0.9392\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17238 to 0.15874, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "8s - loss: 0.1682 - acc: 0.9395 - val_loss: 0.1587 - val_acc: 0.9417\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15874 to 0.14941, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.1543 - acc: 0.9438 - val_loss: 0.1494 - val_acc: 0.9429\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: val_loss improved from 0.14941 to 0.14193, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.1441 - acc: 0.9468 - val_loss: 0.1419 - val_acc: 0.9456\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14193 to 0.13399, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.1355 - acc: 0.9498 - val_loss: 0.1340 - val_acc: 0.9496\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13399 to 0.12796, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.1290 - acc: 0.9520 - val_loss: 0.1280 - val_acc: 0.9513\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12796 to 0.12684, saving model to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "9s - loss: 0.1236 - acc: 0.9540 - val_loss: 0.1268 - val_acc: 0.9523\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1180 - acc: 0.9561 - val_loss: 0.1403 - val_acc: 0.9462\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1137 - acc: 0.9577 - val_loss: 0.1305 - val_acc: 0.9524\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_102_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03574, saving model to ../models/cnn_attention_random_tox_v2_102_probs_model.h5\n",
      "9s - loss: 0.0306 - acc: 0.9596 - val_loss: 0.0357 - val_acc: 0.9527\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03574 to 0.03462, saving model to ../models/cnn_attention_random_tox_v2_102_probs_model.h5\n",
      "8s - loss: 0.0294 - acc: 0.9612 - val_loss: 0.0346 - val_acc: 0.9535\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03462 to 0.03341, saving model to ../models/cnn_attention_random_tox_v2_102_probs_model.h5\n",
      "8s - loss: 0.0281 - acc: 0.9633 - val_loss: 0.0334 - val_acc: 0.9544\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03341 to 0.03341, saving model to ../models/cnn_attention_random_tox_v2_102_probs_model.h5\n",
      "8s - loss: 0.0270 - acc: 0.9648 - val_loss: 0.0334 - val_acc: 0.9547\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.0258 - acc: 0.9669 - val_loss: 0.0338 - val_acc: 0.9544\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0247 - acc: 0.9686 - val_loss: 0.0575 - val_acc: 0.9169\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23646, saving model to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "9s - loss: 0.3009 - acc: 0.9061 - val_loss: 0.2365 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23646 to 0.19300, saving model to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "8s - loss: 0.2163 - acc: 0.9168 - val_loss: 0.1930 - val_acc: 0.9312\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19300 to 0.16934, saving model to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "9s - loss: 0.1833 - acc: 0.9348 - val_loss: 0.1693 - val_acc: 0.9395\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16934 to 0.15146, saving model to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "9s - loss: 0.1626 - acc: 0.9406 - val_loss: 0.1515 - val_acc: 0.9436\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15146 to 0.14175, saving model to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "8s - loss: 0.1493 - acc: 0.9452 - val_loss: 0.1417 - val_acc: 0.9465\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14175 to 0.13484, saving model to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "9s - loss: 0.1400 - acc: 0.9480 - val_loss: 0.1348 - val_acc: 0.9494\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1327 - acc: 0.9506 - val_loss: 0.1557 - val_acc: 0.9465\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1267 - acc: 0.9539 - val_loss: 0.1468 - val_acc: 0.9494\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_103_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03622, saving model to ../models/cnn_attention_random_tox_v2_103_probs_model.h5\n",
      "9s - loss: 0.0340 - acc: 0.9556 - val_loss: 0.0362 - val_acc: 0.9537\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03622 to 0.03609, saving model to ../models/cnn_attention_random_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0329 - acc: 0.9569 - val_loss: 0.0361 - val_acc: 0.9541\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03609 to 0.03431, saving model to ../models/cnn_attention_random_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0316 - acc: 0.9597 - val_loss: 0.0343 - val_acc: 0.9559\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0303 - acc: 0.9617 - val_loss: 0.0347 - val_acc: 0.9551\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03431 to 0.03364, saving model to ../models/cnn_attention_random_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0293 - acc: 0.9630 - val_loss: 0.0336 - val_acc: 0.9569\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0283 - acc: 0.9648 - val_loss: 0.0371 - val_acc: 0.9500\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.0272 - acc: 0.9664 - val_loss: 0.0342 - val_acc: 0.9549\n",
      "Epoch 00006: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22698, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.2921 - acc: 0.9065 - val_loss: 0.2270 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22698 to 0.18835, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.2075 - acc: 0.9187 - val_loss: 0.1884 - val_acc: 0.9327\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18835 to 0.16256, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1776 - acc: 0.9359 - val_loss: 0.1626 - val_acc: 0.9410\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16256 to 0.15306, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1599 - acc: 0.9418 - val_loss: 0.1531 - val_acc: 0.9413\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15306 to 0.13938, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1474 - acc: 0.9451 - val_loss: 0.1394 - val_acc: 0.9477\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13938 to 0.13508, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1379 - acc: 0.9483 - val_loss: 0.1351 - val_acc: 0.9498\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13508 to 0.13326, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1305 - acc: 0.9514 - val_loss: 0.1333 - val_acc: 0.9491\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13326 to 0.12553, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1245 - acc: 0.9535 - val_loss: 0.1255 - val_acc: 0.9532\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1198 - acc: 0.9554 - val_loss: 0.1371 - val_acc: 0.9476\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12553 to 0.12264, saving model to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "9s - loss: 0.1152 - acc: 0.9573 - val_loss: 0.1226 - val_acc: 0.9547\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1118 - acc: 0.9591 - val_loss: 0.1267 - val_acc: 0.9507\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1074 - acc: 0.9602 - val_loss: 0.1239 - val_acc: 0.9548\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_104_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03335, saving model to ../models/cnn_attention_random_tox_v2_104_probs_model.h5\n",
      "9s - loss: 0.0290 - acc: 0.9623 - val_loss: 0.0334 - val_acc: 0.9564\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0278 - acc: 0.9642 - val_loss: 0.0356 - val_acc: 0.9520\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0267 - acc: 0.9656 - val_loss: 0.0338 - val_acc: 0.9560\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23888, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.3023 - acc: 0.9067 - val_loss: 0.2389 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23888 to 0.20209, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.2203 - acc: 0.9104 - val_loss: 0.2021 - val_acc: 0.9222\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20209 to 0.17918, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "8s - loss: 0.1916 - acc: 0.9298 - val_loss: 0.1792 - val_acc: 0.9374\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17918 to 0.16289, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.1737 - acc: 0.9375 - val_loss: 0.1629 - val_acc: 0.9403\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16289 to 0.14953, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.1586 - acc: 0.9417 - val_loss: 0.1495 - val_acc: 0.9442\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14953 to 0.14188, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.1484 - acc: 0.9453 - val_loss: 0.1419 - val_acc: 0.9470\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14188 to 0.13983, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.1403 - acc: 0.9471 - val_loss: 0.1398 - val_acc: 0.9482\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13983 to 0.13293, saving model to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "9s - loss: 0.1333 - acc: 0.9502 - val_loss: 0.1329 - val_acc: 0.9501\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1280 - acc: 0.9523 - val_loss: 0.1475 - val_acc: 0.9509\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1227 - acc: 0.9542 - val_loss: 0.1763 - val_acc: 0.9461\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_105_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03532, saving model to ../models/cnn_attention_random_tox_v2_105_probs_model.h5\n",
      "9s - loss: 0.0332 - acc: 0.9558 - val_loss: 0.0353 - val_acc: 0.9542\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0319 - acc: 0.9582 - val_loss: 0.0434 - val_acc: 0.9455\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03532 to 0.03487, saving model to ../models/cnn_attention_random_tox_v2_105_probs_model.h5\n",
      "8s - loss: 0.0307 - acc: 0.9598 - val_loss: 0.0349 - val_acc: 0.9542\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0295 - acc: 0.9614 - val_loss: 0.0379 - val_acc: 0.9504\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03487 to 0.03414, saving model to ../models/cnn_attention_random_tox_v2_105_probs_model.h5\n",
      "9s - loss: 0.0284 - acc: 0.9633 - val_loss: 0.0341 - val_acc: 0.9546\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.03414 to 0.03392, saving model to ../models/cnn_attention_random_tox_v2_105_probs_model.h5\n",
      "8s - loss: 0.0275 - acc: 0.9647 - val_loss: 0.0339 - val_acc: 0.9554\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.0263 - acc: 0.9667 - val_loss: 0.0343 - val_acc: 0.9552\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.0255 - acc: 0.9679 - val_loss: 0.0348 - val_acc: 0.9542\n",
      "Epoch 00007: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23250, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "10s - loss: 0.3000 - acc: 0.9054 - val_loss: 0.2325 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23250 to 0.19182, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "9s - loss: 0.2133 - acc: 0.9126 - val_loss: 0.1918 - val_acc: 0.9268\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19182 to 0.18143, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "9s - loss: 0.1861 - acc: 0.9325 - val_loss: 0.1814 - val_acc: 0.9349\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18143 to 0.16083, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "9s - loss: 0.1709 - acc: 0.9396 - val_loss: 0.1608 - val_acc: 0.9403\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16083 to 0.15215, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "9s - loss: 0.1567 - acc: 0.9427 - val_loss: 0.1521 - val_acc: 0.9409\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15215 to 0.13974, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "9s - loss: 0.1455 - acc: 0.9465 - val_loss: 0.1397 - val_acc: 0.9474\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13974 to 0.13246, saving model to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "9s - loss: 0.1367 - acc: 0.9499 - val_loss: 0.1325 - val_acc: 0.9496\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1295 - acc: 0.9520 - val_loss: 0.1335 - val_acc: 0.9488\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1236 - acc: 0.9535 - val_loss: 0.1355 - val_acc: 0.9485\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_106_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03977, saving model to ../models/cnn_attention_random_tox_v2_106_probs_model.h5\n",
      "9s - loss: 0.0333 - acc: 0.9558 - val_loss: 0.0398 - val_acc: 0.9463\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03977 to 0.03432, saving model to ../models/cnn_attention_random_tox_v2_106_probs_model.h5\n",
      "8s - loss: 0.0318 - acc: 0.9580 - val_loss: 0.0343 - val_acc: 0.9535\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03432 to 0.03398, saving model to ../models/cnn_attention_random_tox_v2_106_probs_model.h5\n",
      "8s - loss: 0.0306 - acc: 0.9601 - val_loss: 0.0340 - val_acc: 0.9545\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0294 - acc: 0.9617 - val_loss: 0.0437 - val_acc: 0.9390\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.0282 - acc: 0.9637 - val_loss: 0.0369 - val_acc: 0.9494\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24099, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "10s - loss: 0.3019 - acc: 0.9056 - val_loss: 0.2410 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24099 to 0.19505, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.2187 - acc: 0.9141 - val_loss: 0.1951 - val_acc: 0.9308\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19505 to 0.17522, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1862 - acc: 0.9333 - val_loss: 0.1752 - val_acc: 0.9380\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17522 to 0.15625, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1667 - acc: 0.9389 - val_loss: 0.1562 - val_acc: 0.9427\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15625 to 0.14620, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1527 - acc: 0.9438 - val_loss: 0.1462 - val_acc: 0.9441\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14620 to 0.13899, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1425 - acc: 0.9473 - val_loss: 0.1390 - val_acc: 0.9469\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13899 to 0.13796, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1340 - acc: 0.9503 - val_loss: 0.1380 - val_acc: 0.9488\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1277 - acc: 0.9529 - val_loss: 0.1410 - val_acc: 0.9495\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13796 to 0.13783, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1223 - acc: 0.9551 - val_loss: 0.1378 - val_acc: 0.9498\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.1184 - acc: 0.9568 - val_loss: 0.1419 - val_acc: 0.9490\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.13783 to 0.12318, saving model to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "9s - loss: 0.1141 - acc: 0.9583 - val_loss: 0.1232 - val_acc: 0.9550\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1106 - acc: 0.9595 - val_loss: 0.1340 - val_acc: 0.9538\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "8s - loss: 0.1070 - acc: 0.9602 - val_loss: 0.1447 - val_acc: 0.9460\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_107_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03706, saving model to ../models/cnn_attention_random_tox_v2_107_probs_model.h5\n",
      "9s - loss: 0.0292 - acc: 0.9620 - val_loss: 0.0371 - val_acc: 0.9526\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03706 to 0.03273, saving model to ../models/cnn_attention_random_tox_v2_107_probs_model.h5\n",
      "8s - loss: 0.0280 - acc: 0.9637 - val_loss: 0.0327 - val_acc: 0.9568\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0270 - acc: 0.9650 - val_loss: 0.0327 - val_acc: 0.9573\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03273 to 0.03260, saving model to ../models/cnn_attention_random_tox_v2_107_probs_model.h5\n",
      "8s - loss: 0.0258 - acc: 0.9670 - val_loss: 0.0326 - val_acc: 0.9579\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03260 to 0.03255, saving model to ../models/cnn_attention_random_tox_v2_107_probs_model.h5\n",
      "8s - loss: 0.0249 - acc: 0.9686 - val_loss: 0.0326 - val_acc: 0.9575\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0238 - acc: 0.9700 - val_loss: 0.0333 - val_acc: 0.9563\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.0228 - acc: 0.9718 - val_loss: 0.0336 - val_acc: 0.9560\n",
      "Epoch 00006: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23566, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "10s - loss: 0.3026 - acc: 0.9048 - val_loss: 0.2357 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23566 to 0.19070, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "8s - loss: 0.2140 - acc: 0.9158 - val_loss: 0.1907 - val_acc: 0.9330\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19070 to 0.16971, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "9s - loss: 0.1814 - acc: 0.9349 - val_loss: 0.1697 - val_acc: 0.9396\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16971 to 0.15227, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "9s - loss: 0.1626 - acc: 0.9411 - val_loss: 0.1523 - val_acc: 0.9432\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.1491 - acc: 0.9450 - val_loss: 0.1591 - val_acc: 0.9365\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15227 to 0.13440, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "9s - loss: 0.1394 - acc: 0.9484 - val_loss: 0.1344 - val_acc: 0.9491\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13440 to 0.12953, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "9s - loss: 0.1317 - acc: 0.9510 - val_loss: 0.1295 - val_acc: 0.9503\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12953 to 0.12585, saving model to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "9s - loss: 0.1254 - acc: 0.9536 - val_loss: 0.1259 - val_acc: 0.9524\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1205 - acc: 0.9556 - val_loss: 0.1385 - val_acc: 0.9491\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1161 - acc: 0.9571 - val_loss: 0.1356 - val_acc: 0.9509\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_108_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03386, saving model to ../models/cnn_attention_random_tox_v2_108_probs_model.h5\n",
      "10s - loss: 0.0316 - acc: 0.9582 - val_loss: 0.0339 - val_acc: 0.9551\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0303 - acc: 0.9607 - val_loss: 0.0376 - val_acc: 0.9508\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0292 - acc: 0.9625 - val_loss: 0.0359 - val_acc: 0.9526\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24360, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "11s - loss: 0.3059 - acc: 0.9059 - val_loss: 0.2436 - val_acc: 0.9078\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: val_loss improved from 0.24360 to 0.20368, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "9s - loss: 0.2242 - acc: 0.9081 - val_loss: 0.2037 - val_acc: 0.9203\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20368 to 0.18006, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "9s - loss: 0.1940 - acc: 0.9299 - val_loss: 0.1801 - val_acc: 0.9371\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18006 to 0.16100, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "9s - loss: 0.1741 - acc: 0.9380 - val_loss: 0.1610 - val_acc: 0.9414\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16100 to 0.15103, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "9s - loss: 0.1584 - acc: 0.9414 - val_loss: 0.1510 - val_acc: 0.9443\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15103 to 0.14574, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "9s - loss: 0.1483 - acc: 0.9447 - val_loss: 0.1457 - val_acc: 0.9469\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14574 to 0.14260, saving model to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "9s - loss: 0.1405 - acc: 0.9478 - val_loss: 0.1426 - val_acc: 0.9480\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1331 - acc: 0.9505 - val_loss: 0.1568 - val_acc: 0.9457\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1278 - acc: 0.9520 - val_loss: 0.1484 - val_acc: 0.9503\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v2_109_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03517, saving model to ../models/cnn_attention_random_tox_v2_109_probs_model.h5\n",
      "10s - loss: 0.0344 - acc: 0.9538 - val_loss: 0.0352 - val_acc: 0.9530\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03517 to 0.03475, saving model to ../models/cnn_attention_random_tox_v2_109_probs_model.h5\n",
      "9s - loss: 0.0329 - acc: 0.9560 - val_loss: 0.0347 - val_acc: 0.9542\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0317 - acc: 0.9580 - val_loss: 0.0350 - val_acc: 0.9540\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0306 - acc: 0.9598 - val_loss: 0.0376 - val_acc: 0.9500\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(100, 110):\n",
    "\n",
    "    MODEL_NAME = 'cnn_attention_random_tox_v2_{}'.format(i)\n",
    "    debias_random_model = AttentionToxModel()\n",
    "    debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96133463467952296"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "debias_random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23399, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "10s - loss: 0.3010 - acc: 0.9033 - val_loss: 0.2340 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23399 to 0.19310, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.2146 - acc: 0.9146 - val_loss: 0.1931 - val_acc: 0.9324\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19310 to 0.16974, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1832 - acc: 0.9344 - val_loss: 0.1697 - val_acc: 0.9390\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16974 to 0.15464, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1645 - acc: 0.9401 - val_loss: 0.1546 - val_acc: 0.9430\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15464 to 0.15019, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "9s - loss: 0.1522 - acc: 0.9436 - val_loss: 0.1502 - val_acc: 0.9453\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15019 to 0.13878, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "9s - loss: 0.1427 - acc: 0.9470 - val_loss: 0.1388 - val_acc: 0.9478\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13878 to 0.13176, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1353 - acc: 0.9492 - val_loss: 0.1318 - val_acc: 0.9505\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13176 to 0.13161, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1290 - acc: 0.9519 - val_loss: 0.1316 - val_acc: 0.9492\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13161 to 0.12581, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1236 - acc: 0.9537 - val_loss: 0.1258 - val_acc: 0.9529\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12581 to 0.12457, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1194 - acc: 0.9560 - val_loss: 0.1246 - val_acc: 0.9538\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12457 to 0.12268, saving model to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "8s - loss: 0.1151 - acc: 0.9572 - val_loss: 0.1227 - val_acc: 0.9547\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "8s - loss: 0.1111 - acc: 0.9589 - val_loss: 0.1382 - val_acc: 0.9471\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "8s - loss: 0.1076 - acc: 0.9601 - val_loss: 0.1409 - val_acc: 0.9459\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_100_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03431, saving model to ../models/cnn_attention_wiki_tox_v2_100_probs_model.h5\n",
      "9s - loss: 0.0292 - acc: 0.9615 - val_loss: 0.0343 - val_acc: 0.9541\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03431 to 0.03372, saving model to ../models/cnn_attention_wiki_tox_v2_100_probs_model.h5\n",
      "8s - loss: 0.0279 - acc: 0.9637 - val_loss: 0.0337 - val_acc: 0.9557\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0267 - acc: 0.9652 - val_loss: 0.0338 - val_acc: 0.9563\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0255 - acc: 0.9678 - val_loss: 0.0339 - val_acc: 0.9557\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23677, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "11s - loss: 0.3051 - acc: 0.9031 - val_loss: 0.2368 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23677 to 0.19440, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.2133 - acc: 0.9164 - val_loss: 0.1944 - val_acc: 0.9283\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19440 to 0.16531, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1765 - acc: 0.9361 - val_loss: 0.1653 - val_acc: 0.9402\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16531 to 0.15311, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1592 - acc: 0.9417 - val_loss: 0.1531 - val_acc: 0.9436\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15311 to 0.14192, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1475 - acc: 0.9455 - val_loss: 0.1419 - val_acc: 0.9462\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1385 - acc: 0.9484 - val_loss: 0.1434 - val_acc: 0.9480\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14192 to 0.12951, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1311 - acc: 0.9513 - val_loss: 0.1295 - val_acc: 0.9508\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12951 to 0.12612, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1256 - acc: 0.9537 - val_loss: 0.1261 - val_acc: 0.9521\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12612 to 0.12316, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1208 - acc: 0.9557 - val_loss: 0.1232 - val_acc: 0.9534\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.1159 - acc: 0.9574 - val_loss: 0.1253 - val_acc: 0.9516\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12316 to 0.12110, saving model to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "8s - loss: 0.1121 - acc: 0.9589 - val_loss: 0.1211 - val_acc: 0.9540\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "8s - loss: 0.1081 - acc: 0.9602 - val_loss: 0.1237 - val_acc: 0.9557\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "8s - loss: 0.1049 - acc: 0.9613 - val_loss: 0.1211 - val_acc: 0.9560\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_101_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03384, saving model to ../models/cnn_attention_wiki_tox_v2_101_probs_model.h5\n",
      "9s - loss: 0.0283 - acc: 0.9634 - val_loss: 0.0338 - val_acc: 0.9552\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0273 - acc: 0.9644 - val_loss: 0.0350 - val_acc: 0.9530\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0260 - acc: 0.9668 - val_loss: 0.0357 - val_acc: 0.9550\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24799, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "11s - loss: 0.3124 - acc: 0.9026 - val_loss: 0.2480 - val_acc: 0.9045\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: val_loss improved from 0.24799 to 0.20498, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "8s - loss: 0.2270 - acc: 0.9086 - val_loss: 0.2050 - val_acc: 0.9271\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20498 to 0.17704, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "8s - loss: 0.1941 - acc: 0.9314 - val_loss: 0.1770 - val_acc: 0.9359\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17704 to 0.17584, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "8s - loss: 0.1725 - acc: 0.9380 - val_loss: 0.1758 - val_acc: 0.9404\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1578 - acc: 0.9419 - val_loss: 0.1905 - val_acc: 0.9395\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.17584 to 0.16184, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "8s - loss: 0.1467 - acc: 0.9459 - val_loss: 0.1618 - val_acc: 0.9445\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.16184 to 0.15477, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "8s - loss: 0.1389 - acc: 0.9483 - val_loss: 0.1548 - val_acc: 0.9486\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.15477 to 0.15299, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "9s - loss: 0.1315 - acc: 0.9511 - val_loss: 0.1530 - val_acc: 0.9498\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1267 - acc: 0.9526 - val_loss: 0.1723 - val_acc: 0.9454\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.15299 to 0.12575, saving model to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "8s - loss: 0.1215 - acc: 0.9547 - val_loss: 0.1257 - val_acc: 0.9532\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "8s - loss: 0.1174 - acc: 0.9561 - val_loss: 0.1387 - val_acc: 0.9522\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "8s - loss: 0.1137 - acc: 0.9575 - val_loss: 0.1305 - val_acc: 0.9536\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_102_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03583, saving model to ../models/cnn_attention_wiki_tox_v2_102_probs_model.h5\n",
      "10s - loss: 0.0308 - acc: 0.9596 - val_loss: 0.0358 - val_acc: 0.9545\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03583 to 0.03527, saving model to ../models/cnn_attention_wiki_tox_v2_102_probs_model.h5\n",
      "8s - loss: 0.0296 - acc: 0.9612 - val_loss: 0.0353 - val_acc: 0.9541\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03527 to 0.03485, saving model to ../models/cnn_attention_wiki_tox_v2_102_probs_model.h5\n",
      "8s - loss: 0.0285 - acc: 0.9635 - val_loss: 0.0348 - val_acc: 0.9529\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03485 to 0.03386, saving model to ../models/cnn_attention_wiki_tox_v2_102_probs_model.h5\n",
      "8s - loss: 0.0274 - acc: 0.9649 - val_loss: 0.0339 - val_acc: 0.9548\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.0261 - acc: 0.9671 - val_loss: 0.0395 - val_acc: 0.9471\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0250 - acc: 0.9688 - val_loss: 0.0358 - val_acc: 0.9513\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.25384, saving model to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "11s - loss: 0.3109 - acc: 0.9024 - val_loss: 0.2538 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.25384 to 0.21267, saving model to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "8s - loss: 0.2321 - acc: 0.9042 - val_loss: 0.2127 - val_acc: 0.9124\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21267 to 0.19625, saving model to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "8s - loss: 0.2027 - acc: 0.9241 - val_loss: 0.1963 - val_acc: 0.9332\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.1866 - acc: 0.9345 - val_loss: 0.1966 - val_acc: 0.9251\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.19625 to 0.16198, saving model to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "8s - loss: 0.1728 - acc: 0.9377 - val_loss: 0.1620 - val_acc: 0.9425\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.16198 to 0.16072, saving model to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "9s - loss: 0.1602 - acc: 0.9413 - val_loss: 0.1607 - val_acc: 0.9422\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.16072 to 0.15702, saving model to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "9s - loss: 0.1512 - acc: 0.9438 - val_loss: 0.1570 - val_acc: 0.9450\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1433 - acc: 0.9474 - val_loss: 0.1636 - val_acc: 0.9423\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1378 - acc: 0.9495 - val_loss: 0.1686 - val_acc: 0.9416\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_103_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.04536, saving model to ../models/cnn_attention_wiki_tox_v2_103_probs_model.h5\n",
      "10s - loss: 0.0371 - acc: 0.9513 - val_loss: 0.0454 - val_acc: 0.9460\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.04536 to 0.04535, saving model to ../models/cnn_attention_wiki_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0357 - acc: 0.9532 - val_loss: 0.0454 - val_acc: 0.9446\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.04535 to 0.03808, saving model to ../models/cnn_attention_wiki_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0342 - acc: 0.9555 - val_loss: 0.0381 - val_acc: 0.9510\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03808 to 0.03625, saving model to ../models/cnn_attention_wiki_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0332 - acc: 0.9571 - val_loss: 0.0363 - val_acc: 0.9524\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03625 to 0.03579, saving model to ../models/cnn_attention_wiki_tox_v2_103_probs_model.h5\n",
      "8s - loss: 0.0321 - acc: 0.9588 - val_loss: 0.0358 - val_acc: 0.9531\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0311 - acc: 0.9606 - val_loss: 0.0410 - val_acc: 0.9457\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.0299 - acc: 0.9623 - val_loss: 0.0372 - val_acc: 0.9514\n",
      "Epoch 00006: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.25084, saving model to ../models/cnn_attention_wiki_tox_v2_104_model.h5\n",
      "11s - loss: 0.3122 - acc: 0.9015 - val_loss: 0.2508 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.25084 to 0.21378, saving model to ../models/cnn_attention_wiki_tox_v2_104_model.h5\n",
      "8s - loss: 0.2288 - acc: 0.9075 - val_loss: 0.2138 - val_acc: 0.9211\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21378 to 0.18479, saving model to ../models/cnn_attention_wiki_tox_v2_104_model.h5\n",
      "8s - loss: 0.1979 - acc: 0.9288 - val_loss: 0.1848 - val_acc: 0.9343\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18479 to 0.17259, saving model to ../models/cnn_attention_wiki_tox_v2_104_model.h5\n",
      "8s - loss: 0.1783 - acc: 0.9365 - val_loss: 0.1726 - val_acc: 0.9389\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: val_loss improved from 0.17259 to 0.15353, saving model to ../models/cnn_attention_wiki_tox_v2_104_model.h5\n",
      "8s - loss: 0.1635 - acc: 0.9407 - val_loss: 0.1535 - val_acc: 0.9429\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.1523 - acc: 0.9441 - val_loss: 0.1544 - val_acc: 0.9430\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.1424 - acc: 0.9468 - val_loss: 0.1586 - val_acc: 0.9447\n",
      "Epoch 00006: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_104_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03869, saving model to ../models/cnn_attention_wiki_tox_v2_104_probs_model.h5\n",
      "10s - loss: 0.0382 - acc: 0.9495 - val_loss: 0.0387 - val_acc: 0.9479\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03869 to 0.03780, saving model to ../models/cnn_attention_wiki_tox_v2_104_probs_model.h5\n",
      "8s - loss: 0.0364 - acc: 0.9521 - val_loss: 0.0378 - val_acc: 0.9502\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0351 - acc: 0.9541 - val_loss: 0.0543 - val_acc: 0.9353\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03780 to 0.03561, saving model to ../models/cnn_attention_wiki_tox_v2_104_probs_model.h5\n",
      "8s - loss: 0.0337 - acc: 0.9564 - val_loss: 0.0356 - val_acc: 0.9530\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03561 to 0.03544, saving model to ../models/cnn_attention_wiki_tox_v2_104_probs_model.h5\n",
      "8s - loss: 0.0325 - acc: 0.9581 - val_loss: 0.0354 - val_acc: 0.9534\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.03544 to 0.03497, saving model to ../models/cnn_attention_wiki_tox_v2_104_probs_model.h5\n",
      "8s - loss: 0.0314 - acc: 0.9603 - val_loss: 0.0350 - val_acc: 0.9544\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.0301 - acc: 0.9623 - val_loss: 0.0371 - val_acc: 0.9518\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.03497 to 0.03468, saving model to ../models/cnn_attention_wiki_tox_v2_104_probs_model.h5\n",
      "8s - loss: 0.0293 - acc: 0.9637 - val_loss: 0.0347 - val_acc: 0.9547\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.0282 - acc: 0.9652 - val_loss: 0.0357 - val_acc: 0.9537\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "8s - loss: 0.0272 - acc: 0.9665 - val_loss: 0.0358 - val_acc: 0.9546\n",
      "Epoch 00009: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24355, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "11s - loss: 0.3067 - acc: 0.9033 - val_loss: 0.2435 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24355 to 0.19893, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "8s - loss: 0.2222 - acc: 0.9111 - val_loss: 0.1989 - val_acc: 0.9283\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19893 to 0.17717, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "8s - loss: 0.1890 - acc: 0.9330 - val_loss: 0.1772 - val_acc: 0.9345\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17717 to 0.16583, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "9s - loss: 0.1689 - acc: 0.9391 - val_loss: 0.1658 - val_acc: 0.9374\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16583 to 0.15117, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "9s - loss: 0.1541 - acc: 0.9433 - val_loss: 0.1512 - val_acc: 0.9453\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15117 to 0.14096, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "9s - loss: 0.1436 - acc: 0.9466 - val_loss: 0.1410 - val_acc: 0.9479\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14096 to 0.13519, saving model to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "9s - loss: 0.1360 - acc: 0.9491 - val_loss: 0.1352 - val_acc: 0.9504\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1296 - acc: 0.9511 - val_loss: 0.1368 - val_acc: 0.9491\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1250 - acc: 0.9533 - val_loss: 0.1609 - val_acc: 0.9418\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_105_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03594, saving model to ../models/cnn_attention_wiki_tox_v2_105_probs_model.h5\n",
      "10s - loss: 0.0337 - acc: 0.9552 - val_loss: 0.0359 - val_acc: 0.9529\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0323 - acc: 0.9576 - val_loss: 0.0384 - val_acc: 0.9499\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0312 - acc: 0.9592 - val_loss: 0.0427 - val_acc: 0.9448\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23316, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "12s - loss: 0.2993 - acc: 0.9033 - val_loss: 0.2332 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23316 to 0.18791, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.2103 - acc: 0.9196 - val_loss: 0.1879 - val_acc: 0.9332\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18791 to 0.16916, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1764 - acc: 0.9365 - val_loss: 0.1692 - val_acc: 0.9374\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16916 to 0.15096, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1591 - acc: 0.9421 - val_loss: 0.1510 - val_acc: 0.9449\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15096 to 0.14373, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1479 - acc: 0.9458 - val_loss: 0.1437 - val_acc: 0.9474\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14373 to 0.13700, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1394 - acc: 0.9490 - val_loss: 0.1370 - val_acc: 0.9504\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13700 to 0.13388, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1329 - acc: 0.9517 - val_loss: 0.1339 - val_acc: 0.9503\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13388 to 0.12808, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1272 - acc: 0.9536 - val_loss: 0.1281 - val_acc: 0.9538\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1226 - acc: 0.9557 - val_loss: 0.1306 - val_acc: 0.9506\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12808 to 0.12256, saving model to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "9s - loss: 0.1175 - acc: 0.9570 - val_loss: 0.1226 - val_acc: 0.9542\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1135 - acc: 0.9587 - val_loss: 0.1274 - val_acc: 0.9520\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1097 - acc: 0.9599 - val_loss: 0.1231 - val_acc: 0.9533\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_106_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 0.03463, saving model to ../models/cnn_attention_wiki_tox_v2_106_probs_model.h5\n",
      "10s - loss: 0.0296 - acc: 0.9614 - val_loss: 0.0346 - val_acc: 0.9536\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0283 - acc: 0.9634 - val_loss: 0.0352 - val_acc: 0.9528\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0271 - acc: 0.9653 - val_loss: 0.0373 - val_acc: 0.9493\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23762, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "12s - loss: 0.3004 - acc: 0.9025 - val_loss: 0.2376 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23762 to 0.19253, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "8s - loss: 0.2159 - acc: 0.9159 - val_loss: 0.1925 - val_acc: 0.9325\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19253 to 0.16597, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "9s - loss: 0.1829 - acc: 0.9356 - val_loss: 0.1660 - val_acc: 0.9396\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16597 to 0.15279, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "9s - loss: 0.1620 - acc: 0.9414 - val_loss: 0.1528 - val_acc: 0.9427\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15279 to 0.14261, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "9s - loss: 0.1502 - acc: 0.9451 - val_loss: 0.1426 - val_acc: 0.9470\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14261 to 0.13635, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "9s - loss: 0.1405 - acc: 0.9484 - val_loss: 0.1363 - val_acc: 0.9492\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13635 to 0.13028, saving model to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "9s - loss: 0.1323 - acc: 0.9509 - val_loss: 0.1303 - val_acc: 0.9505\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1272 - acc: 0.9529 - val_loss: 0.1378 - val_acc: 0.9487\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1219 - acc: 0.9554 - val_loss: 0.1311 - val_acc: 0.9528\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_107_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03569, saving model to ../models/cnn_attention_wiki_tox_v2_107_probs_model.h5\n",
      "10s - loss: 0.0331 - acc: 0.9566 - val_loss: 0.0357 - val_acc: 0.9534\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "8s - loss: 0.0318 - acc: 0.9585 - val_loss: 0.0366 - val_acc: 0.9532\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03569 to 0.03415, saving model to ../models/cnn_attention_wiki_tox_v2_107_probs_model.h5\n",
      "8s - loss: 0.0308 - acc: 0.9600 - val_loss: 0.0341 - val_acc: 0.9551\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03415 to 0.03378, saving model to ../models/cnn_attention_wiki_tox_v2_107_probs_model.h5\n",
      "8s - loss: 0.0295 - acc: 0.9618 - val_loss: 0.0338 - val_acc: 0.9564\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.0284 - acc: 0.9639 - val_loss: 0.0340 - val_acc: 0.9552\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0272 - acc: 0.9660 - val_loss: 0.0349 - val_acc: 0.9534\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24498, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "12s - loss: 0.3074 - acc: 0.9031 - val_loss: 0.2450 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24498 to 0.20068, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.2225 - acc: 0.9115 - val_loss: 0.2007 - val_acc: 0.9274\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20068 to 0.18097, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1905 - acc: 0.9325 - val_loss: 0.1810 - val_acc: 0.9335\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.18097 to 0.16135, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1710 - acc: 0.9382 - val_loss: 0.1613 - val_acc: 0.9408\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16135 to 0.14893, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1571 - acc: 0.9430 - val_loss: 0.1489 - val_acc: 0.9435\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14893 to 0.14136, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1461 - acc: 0.9458 - val_loss: 0.1414 - val_acc: 0.9463\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14136 to 0.13676, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1381 - acc: 0.9483 - val_loss: 0.1368 - val_acc: 0.9481\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13676 to 0.13141, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "8s - loss: 0.1314 - acc: 0.9506 - val_loss: 0.1314 - val_acc: 0.9504\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1254 - acc: 0.9531 - val_loss: 0.1517 - val_acc: 0.9438\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.13141 to 0.12881, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1208 - acc: 0.9551 - val_loss: 0.1288 - val_acc: 0.9523\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12881 to 0.12362, saving model to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "9s - loss: 0.1165 - acc: 0.9558 - val_loss: 0.1236 - val_acc: 0.9536\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1127 - acc: 0.9579 - val_loss: 0.1572 - val_acc: 0.9407\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "9s - loss: 0.1092 - acc: 0.9590 - val_loss: 0.1512 - val_acc: 0.9437\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_108_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.04820, saving model to ../models/cnn_attention_wiki_tox_v2_108_probs_model.h5\n",
      "10s - loss: 0.0295 - acc: 0.9604 - val_loss: 0.0482 - val_acc: 0.9359\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.04820 to 0.03593, saving model to ../models/cnn_attention_wiki_tox_v2_108_probs_model.h5\n",
      "8s - loss: 0.0285 - acc: 0.9622 - val_loss: 0.0359 - val_acc: 0.9519\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03593 to 0.03413, saving model to ../models/cnn_attention_wiki_tox_v2_108_probs_model.h5\n",
      "8s - loss: 0.0272 - acc: 0.9643 - val_loss: 0.0341 - val_acc: 0.9547\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0260 - acc: 0.9664 - val_loss: 0.0357 - val_acc: 0.9513\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03413 to 0.03412, saving model to ../models/cnn_attention_wiki_tox_v2_108_probs_model.h5\n",
      "8s - loss: 0.0248 - acc: 0.9684 - val_loss: 0.0341 - val_acc: 0.9551\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "8s - loss: 0.0237 - acc: 0.9699 - val_loss: 0.0349 - val_acc: 0.9541\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "8s - loss: 0.0227 - acc: 0.9718 - val_loss: 0.0361 - val_acc: 0.9518\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.24268, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "12s - loss: 0.3061 - acc: 0.9027 - val_loss: 0.2427 - val_acc: 0.9045\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.24268 to 0.20433, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "8s - loss: 0.2213 - acc: 0.9101 - val_loss: 0.2043 - val_acc: 0.9219\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.20433 to 0.18082, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1930 - acc: 0.9310 - val_loss: 0.1808 - val_acc: 0.9372\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.1756 - acc: 0.9374 - val_loss: 0.1876 - val_acc: 0.9212\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.18082 to 0.17478, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1613 - acc: 0.9411 - val_loss: 0.1748 - val_acc: 0.9321\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.17478 to 0.15867, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1488 - acc: 0.9451 - val_loss: 0.1587 - val_acc: 0.9382\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.15867 to 0.14386, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1409 - acc: 0.9484 - val_loss: 0.1439 - val_acc: 0.9471\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14386 to 0.13985, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1339 - acc: 0.9504 - val_loss: 0.1399 - val_acc: 0.9483\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1283 - acc: 0.9526 - val_loss: 0.1599 - val_acc: 0.9448\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.13985 to 0.12914, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1231 - acc: 0.9544 - val_loss: 0.1291 - val_acc: 0.9516\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12914 to 0.12690, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1185 - acc: 0.9557 - val_loss: 0.1269 - val_acc: 0.9525\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "8s - loss: 0.1145 - acc: 0.9574 - val_loss: 0.1378 - val_acc: 0.9503\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.12690 to 0.12316, saving model to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "9s - loss: 0.1106 - acc: 0.9586 - val_loss: 0.1232 - val_acc: 0.9539\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "8s - loss: 0.1069 - acc: 0.9597 - val_loss: 0.1254 - val_acc: 0.9521\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "9s - loss: 0.1037 - acc: 0.9614 - val_loss: 0.1423 - val_acc: 0.9497\n",
      "Epoch 00014: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_wiki_tox_v2_109_model.h5\n",
      "Fitting probs model\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03518, saving model to ../models/cnn_attention_wiki_tox_v2_109_probs_model.h5\n",
      "11s - loss: 0.0281 - acc: 0.9628 - val_loss: 0.0352 - val_acc: 0.9533\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03518 to 0.03391, saving model to ../models/cnn_attention_wiki_tox_v2_109_probs_model.h5\n",
      "8s - loss: 0.0265 - acc: 0.9652 - val_loss: 0.0339 - val_acc: 0.9552\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "8s - loss: 0.0254 - acc: 0.9673 - val_loss: 0.0424 - val_acc: 0.9419\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "8s - loss: 0.0240 - acc: 0.9696 - val_loss: 0.0352 - val_acc: 0.9528\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(100, 110):\n",
    "\n",
    "    MODEL_NAME = 'cnn_attention_wiki_tox_v2_{}'.format(i)\n",
    "    wiki_model = AttentionToxModel()\n",
    "    wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96207184914138488"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22985, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "13s - loss: 0.2893 - acc: 0.9067 - val_loss: 0.2299 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22985 to 0.18935, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.2086 - acc: 0.9146 - val_loss: 0.1894 - val_acc: 0.9316\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18935 to 0.17390, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.1841 - acc: 0.9343 - val_loss: 0.1739 - val_acc: 0.9401\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17390 to 0.16515, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.1688 - acc: 0.9408 - val_loss: 0.1651 - val_acc: 0.9363\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16515 to 0.14947, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.1542 - acc: 0.9440 - val_loss: 0.1495 - val_acc: 0.9422\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14947 to 0.14013, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.1424 - acc: 0.9482 - val_loss: 0.1401 - val_acc: 0.9481\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14013 to 0.13232, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.1343 - acc: 0.9504 - val_loss: 0.1323 - val_acc: 0.9510\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13232 to 0.12940, saving model to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "9s - loss: 0.1274 - acc: 0.9537 - val_loss: 0.1294 - val_acc: 0.9516\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1218 - acc: 0.9555 - val_loss: 0.1330 - val_acc: 0.9491\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1176 - acc: 0.9567 - val_loss: 0.1563 - val_acc: 0.9367\n",
      "Epoch 00009: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_100_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03765, saving model to ../models/cnn_attention_debias_tox_v2_100_probs_model.h5\n",
      "11s - loss: 0.0315 - acc: 0.9585 - val_loss: 0.0376 - val_acc: 0.9487\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03765 to 0.03454, saving model to ../models/cnn_attention_debias_tox_v2_100_probs_model.h5\n",
      "9s - loss: 0.0302 - acc: 0.9609 - val_loss: 0.0345 - val_acc: 0.9537\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "9s - loss: 0.0290 - acc: 0.9631 - val_loss: 0.0361 - val_acc: 0.9520\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0279 - acc: 0.9643 - val_loss: 0.0373 - val_acc: 0.9492\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23246, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "13s - loss: 0.3017 - acc: 0.9053 - val_loss: 0.2325 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23246 to 0.19018, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.2140 - acc: 0.9173 - val_loss: 0.1902 - val_acc: 0.9327\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19018 to 0.16837, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1796 - acc: 0.9355 - val_loss: 0.1684 - val_acc: 0.9397\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16837 to 0.15287, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1624 - acc: 0.9408 - val_loss: 0.1529 - val_acc: 0.9433\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15287 to 0.14420, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1508 - acc: 0.9445 - val_loss: 0.1442 - val_acc: 0.9459\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14420 to 0.14227, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1420 - acc: 0.9480 - val_loss: 0.1423 - val_acc: 0.9477\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14227 to 0.13562, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1351 - acc: 0.9505 - val_loss: 0.1356 - val_acc: 0.9503\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13562 to 0.12762, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1294 - acc: 0.9530 - val_loss: 0.1276 - val_acc: 0.9523\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12762 to 0.12563, saving model to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "9s - loss: 0.1242 - acc: 0.9546 - val_loss: 0.1256 - val_acc: 0.9524\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1194 - acc: 0.9566 - val_loss: 0.1268 - val_acc: 0.9536\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1155 - acc: 0.9582 - val_loss: 0.1258 - val_acc: 0.9544\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_101_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03362, saving model to ../models/cnn_attention_debias_tox_v2_101_probs_model.h5\n",
      "11s - loss: 0.0315 - acc: 0.9589 - val_loss: 0.0336 - val_acc: 0.9557\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "9s - loss: 0.0302 - acc: 0.9611 - val_loss: 0.0339 - val_acc: 0.9563\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "9s - loss: 0.0293 - acc: 0.9627 - val_loss: 0.0350 - val_acc: 0.9538\n",
      "Epoch 00002: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23225, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "13s - loss: 0.2982 - acc: 0.9064 - val_loss: 0.2322 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23225 to 0.19489, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.2126 - acc: 0.9156 - val_loss: 0.1949 - val_acc: 0.9330\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19489 to 0.16997, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.1836 - acc: 0.9352 - val_loss: 0.1700 - val_acc: 0.9389\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16997 to 0.15631, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.1644 - acc: 0.9406 - val_loss: 0.1563 - val_acc: 0.9438\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15631 to 0.14458, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.1515 - acc: 0.9449 - val_loss: 0.1446 - val_acc: 0.9459\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: val_loss improved from 0.14458 to 0.13926, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.1416 - acc: 0.9482 - val_loss: 0.1393 - val_acc: 0.9472\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13926 to 0.12939, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.1337 - acc: 0.9512 - val_loss: 0.1294 - val_acc: 0.9510\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.12939 to 0.12623, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "9s - loss: 0.1280 - acc: 0.9529 - val_loss: 0.1262 - val_acc: 0.9527\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.12623 to 0.12381, saving model to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "10s - loss: 0.1219 - acc: 0.9551 - val_loss: 0.1238 - val_acc: 0.9536\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1176 - acc: 0.9570 - val_loss: 0.1263 - val_acc: 0.9531\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1136 - acc: 0.9584 - val_loss: 0.1340 - val_acc: 0.9500\n",
      "Epoch 00010: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_102_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03404, saving model to ../models/cnn_attention_debias_tox_v2_102_probs_model.h5\n",
      "11s - loss: 0.0306 - acc: 0.9599 - val_loss: 0.0340 - val_acc: 0.9551\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "9s - loss: 0.0293 - acc: 0.9619 - val_loss: 0.0392 - val_acc: 0.9475\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03404 to 0.03317, saving model to ../models/cnn_attention_debias_tox_v2_102_probs_model.h5\n",
      "9s - loss: 0.0282 - acc: 0.9634 - val_loss: 0.0332 - val_acc: 0.9569\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0270 - acc: 0.9658 - val_loss: 0.0336 - val_acc: 0.9556\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.0260 - acc: 0.9677 - val_loss: 0.0340 - val_acc: 0.9557\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22559, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "13s - loss: 0.2864 - acc: 0.9066 - val_loss: 0.2256 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22559 to 0.18876, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.2043 - acc: 0.9218 - val_loss: 0.1888 - val_acc: 0.9353\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18876 to 0.16291, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1754 - acc: 0.9373 - val_loss: 0.1629 - val_acc: 0.9415\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16291 to 0.14733, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1573 - acc: 0.9426 - val_loss: 0.1473 - val_acc: 0.9460\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.14733 to 0.13798, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1459 - acc: 0.9469 - val_loss: 0.1380 - val_acc: 0.9487\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13798 to 0.13725, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1369 - acc: 0.9500 - val_loss: 0.1372 - val_acc: 0.9494\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.1300 - acc: 0.9525 - val_loss: 0.1375 - val_acc: 0.9511\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13725 to 0.12445, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1248 - acc: 0.9546 - val_loss: 0.1244 - val_acc: 0.9537\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1198 - acc: 0.9558 - val_loss: 0.1456 - val_acc: 0.9492\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.12445 to 0.12043, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1155 - acc: 0.9577 - val_loss: 0.1204 - val_acc: 0.9552\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1121 - acc: 0.9583 - val_loss: 0.1206 - val_acc: 0.9557\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12043 to 0.12012, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1081 - acc: 0.9599 - val_loss: 0.1201 - val_acc: 0.9559\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.12012 to 0.11844, saving model to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "9s - loss: 0.1046 - acc: 0.9615 - val_loss: 0.1184 - val_acc: 0.9566\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "9s - loss: 0.1016 - acc: 0.9625 - val_loss: 0.1188 - val_acc: 0.9581\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "9s - loss: 0.0981 - acc: 0.9639 - val_loss: 0.1194 - val_acc: 0.9560\n",
      "Epoch 00014: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_103_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03361, saving model to ../models/cnn_attention_debias_tox_v2_103_probs_model.h5\n",
      "12s - loss: 0.0263 - acc: 0.9652 - val_loss: 0.0336 - val_acc: 0.9570\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03361 to 0.03346, saving model to ../models/cnn_attention_debias_tox_v2_103_probs_model.h5\n",
      "9s - loss: 0.0250 - acc: 0.9676 - val_loss: 0.0335 - val_acc: 0.9556\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "9s - loss: 0.0240 - acc: 0.9692 - val_loss: 0.0367 - val_acc: 0.9511\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0227 - acc: 0.9716 - val_loss: 0.0375 - val_acc: 0.9500\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23333, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "13s - loss: 0.2918 - acc: 0.9068 - val_loss: 0.2333 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23333 to 0.19515, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.2137 - acc: 0.9138 - val_loss: 0.1951 - val_acc: 0.9319\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19515 to 0.17140, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1850 - acc: 0.9343 - val_loss: 0.1714 - val_acc: 0.9378\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17140 to 0.16194, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1679 - acc: 0.9395 - val_loss: 0.1619 - val_acc: 0.9429\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16194 to 0.14610, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1528 - acc: 0.9439 - val_loss: 0.1461 - val_acc: 0.9457\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14610 to 0.13976, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1422 - acc: 0.9477 - val_loss: 0.1398 - val_acc: 0.9482\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13976 to 0.13766, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1348 - acc: 0.9497 - val_loss: 0.1377 - val_acc: 0.9490\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13766 to 0.13547, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1284 - acc: 0.9524 - val_loss: 0.1355 - val_acc: 0.9513\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1234 - acc: 0.9542 - val_loss: 0.1400 - val_acc: 0.9483\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.13547 to 0.12618, saving model to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "9s - loss: 0.1185 - acc: 0.9559 - val_loss: 0.1262 - val_acc: 0.9526\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.1142 - acc: 0.9578 - val_loss: 0.1304 - val_acc: 0.9523\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1105 - acc: 0.9593 - val_loss: 0.1535 - val_acc: 0.9472\n",
      "Epoch 00011: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_104_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03430, saving model to ../models/cnn_attention_debias_tox_v2_104_probs_model.h5\n",
      "12s - loss: 0.0298 - acc: 0.9602 - val_loss: 0.0343 - val_acc: 0.9537\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03430 to 0.03361, saving model to ../models/cnn_attention_debias_tox_v2_104_probs_model.h5\n",
      "9s - loss: 0.0287 - acc: 0.9622 - val_loss: 0.0336 - val_acc: 0.9548\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "9s - loss: 0.0275 - acc: 0.9641 - val_loss: 0.0341 - val_acc: 0.9539\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0263 - acc: 0.9659 - val_loss: 0.0338 - val_acc: 0.9543\n",
      "Epoch 00003: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23305, saving model to ../models/cnn_attention_debias_tox_v2_105_model.h5\n",
      "14s - loss: 0.2992 - acc: 0.9044 - val_loss: 0.2331 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23305 to 0.19145, saving model to ../models/cnn_attention_debias_tox_v2_105_model.h5\n",
      "9s - loss: 0.2121 - acc: 0.9160 - val_loss: 0.1915 - val_acc: 0.9334\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19145 to 0.17339, saving model to ../models/cnn_attention_debias_tox_v2_105_model.h5\n",
      "9s - loss: 0.1841 - acc: 0.9353 - val_loss: 0.1734 - val_acc: 0.9396\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17339 to 0.15490, saving model to ../models/cnn_attention_debias_tox_v2_105_model.h5\n",
      "9s - loss: 0.1662 - acc: 0.9400 - val_loss: 0.1549 - val_acc: 0.9435\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.1529 - acc: 0.9441 - val_loss: 0.1617 - val_acc: 0.9409\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15490 to 0.13954, saving model to ../models/cnn_attention_debias_tox_v2_105_model.h5\n",
      "9s - loss: 0.1433 - acc: 0.9469 - val_loss: 0.1395 - val_acc: 0.9487\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.1364 - acc: 0.9498 - val_loss: 0.1782 - val_acc: 0.9405\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1297 - acc: 0.9515 - val_loss: 0.1578 - val_acc: 0.9466\n",
      "Epoch 00007: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_105_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03603, saving model to ../models/cnn_attention_debias_tox_v2_105_probs_model.h5\n",
      "12s - loss: 0.0350 - acc: 0.9537 - val_loss: 0.0360 - val_acc: 0.9530\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "9s - loss: 0.0336 - acc: 0.9557 - val_loss: 0.0361 - val_acc: 0.9535\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03603 to 0.03479, saving model to ../models/cnn_attention_debias_tox_v2_105_probs_model.h5\n",
      "9s - loss: 0.0323 - acc: 0.9583 - val_loss: 0.0348 - val_acc: 0.9545\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0313 - acc: 0.9595 - val_loss: 0.0352 - val_acc: 0.9542\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.03479 to 0.03407, saving model to ../models/cnn_attention_debias_tox_v2_105_probs_model.h5\n",
      "9s - loss: 0.0301 - acc: 0.9619 - val_loss: 0.0341 - val_acc: 0.9551\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.0290 - acc: 0.9633 - val_loss: 0.0363 - val_acc: 0.9527\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.0278 - acc: 0.9652 - val_loss: 0.0365 - val_acc: 0.9528\n",
      "Epoch 00006: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23742, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "14s - loss: 0.3004 - acc: 0.9058 - val_loss: 0.2374 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23742 to 0.19657, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.2185 - acc: 0.9131 - val_loss: 0.1966 - val_acc: 0.9294\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19657 to 0.17610, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1890 - acc: 0.9328 - val_loss: 0.1761 - val_acc: 0.9356\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17610 to 0.17061, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1709 - acc: 0.9392 - val_loss: 0.1706 - val_acc: 0.9335\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.17061 to 0.15252, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1575 - acc: 0.9429 - val_loss: 0.1525 - val_acc: 0.9440\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15252 to 0.14678, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1467 - acc: 0.9463 - val_loss: 0.1468 - val_acc: 0.9452\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.1382 - acc: 0.9498 - val_loss: 0.1472 - val_acc: 0.9468\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14678 to 0.13373, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1324 - acc: 0.9518 - val_loss: 0.1337 - val_acc: 0.9503\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13373 to 0.12898, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1266 - acc: 0.9534 - val_loss: 0.1290 - val_acc: 0.9528\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1218 - acc: 0.9552 - val_loss: 0.1533 - val_acc: 0.9433\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12898 to 0.12675, saving model to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "9s - loss: 0.1173 - acc: 0.9567 - val_loss: 0.1268 - val_acc: 0.9529\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1134 - acc: 0.9584 - val_loss: 0.1330 - val_acc: 0.9506\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "9s - loss: 0.1101 - acc: 0.9597 - val_loss: 0.1523 - val_acc: 0.9434\n",
      "Epoch 00012: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_106_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.04458, saving model to ../models/cnn_attention_debias_tox_v2_106_probs_model.h5\n",
      "12s - loss: 0.0295 - acc: 0.9608 - val_loss: 0.0446 - val_acc: 0.9392\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.04458 to 0.03764, saving model to ../models/cnn_attention_debias_tox_v2_106_probs_model.h5\n",
      "9s - loss: 0.0283 - acc: 0.9628 - val_loss: 0.0376 - val_acc: 0.9496\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002: val_loss did not improve\n",
      "9s - loss: 0.0271 - acc: 0.9652 - val_loss: 0.0412 - val_acc: 0.9425\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03764 to 0.03734, saving model to ../models/cnn_attention_debias_tox_v2_106_probs_model.h5\n",
      "9s - loss: 0.0260 - acc: 0.9668 - val_loss: 0.0373 - val_acc: 0.9490\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.0249 - acc: 0.9684 - val_loss: 0.0387 - val_acc: 0.9466\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.03734 to 0.03426, saving model to ../models/cnn_attention_debias_tox_v2_106_probs_model.h5\n",
      "9s - loss: 0.0239 - acc: 0.9697 - val_loss: 0.0343 - val_acc: 0.9548\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.0227 - acc: 0.9718 - val_loss: 0.0360 - val_acc: 0.9522\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.0218 - acc: 0.9729 - val_loss: 0.0376 - val_acc: 0.9505\n",
      "Epoch 00007: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23008, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "14s - loss: 0.2955 - acc: 0.9064 - val_loss: 0.2301 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23008 to 0.18941, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "9s - loss: 0.2099 - acc: 0.9125 - val_loss: 0.1894 - val_acc: 0.9283\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18941 to 0.17412, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1850 - acc: 0.9340 - val_loss: 0.1741 - val_acc: 0.9401\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17412 to 0.16288, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1694 - acc: 0.9399 - val_loss: 0.1629 - val_acc: 0.9435\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.16288 to 0.15311, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1552 - acc: 0.9437 - val_loss: 0.1531 - val_acc: 0.9432\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.1451 - acc: 0.9470 - val_loss: 0.1662 - val_acc: 0.9395\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.15311 to 0.14513, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "9s - loss: 0.1365 - acc: 0.9500 - val_loss: 0.1451 - val_acc: 0.9471\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14513 to 0.13152, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1304 - acc: 0.9513 - val_loss: 0.1315 - val_acc: 0.9505\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13152 to 0.12776, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1247 - acc: 0.9534 - val_loss: 0.1278 - val_acc: 0.9529\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "10s - loss: 0.1198 - acc: 0.9552 - val_loss: 0.1385 - val_acc: 0.9506\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12776 to 0.12229, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1153 - acc: 0.9568 - val_loss: 0.1223 - val_acc: 0.9548\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.1115 - acc: 0.9581 - val_loss: 0.1259 - val_acc: 0.9541\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.12229 to 0.12181, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1080 - acc: 0.9592 - val_loss: 0.1218 - val_acc: 0.9545\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss improved from 0.12181 to 0.11969, saving model to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "10s - loss: 0.1036 - acc: 0.9608 - val_loss: 0.1197 - val_acc: 0.9558\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "10s - loss: 0.1001 - acc: 0.9622 - val_loss: 0.1272 - val_acc: 0.9529\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "10s - loss: 0.0968 - acc: 0.9633 - val_loss: 0.1266 - val_acc: 0.9522\n",
      "Epoch 00015: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_107_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03445, saving model to ../models/cnn_attention_debias_tox_v2_107_probs_model.h5\n",
      "12s - loss: 0.0262 - acc: 0.9650 - val_loss: 0.0345 - val_acc: 0.9535\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss did not improve\n",
      "9s - loss: 0.0248 - acc: 0.9674 - val_loss: 0.0422 - val_acc: 0.9403\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03445 to 0.03398, saving model to ../models/cnn_attention_debias_tox_v2_107_probs_model.h5\n",
      "9s - loss: 0.0238 - acc: 0.9689 - val_loss: 0.0340 - val_acc: 0.9559\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03398 to 0.03383, saving model to ../models/cnn_attention_debias_tox_v2_107_probs_model.h5\n",
      "9s - loss: 0.0226 - acc: 0.9707 - val_loss: 0.0338 - val_acc: 0.9563\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.0217 - acc: 0.9725 - val_loss: 0.0355 - val_acc: 0.9531\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.0207 - acc: 0.9739 - val_loss: 0.0367 - val_acc: 0.9511\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23740, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "14s - loss: 0.3038 - acc: 0.9057 - val_loss: 0.2374 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23740 to 0.19827, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "9s - loss: 0.2176 - acc: 0.9126 - val_loss: 0.1983 - val_acc: 0.9272\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.19827 to 0.17292, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "10s - loss: 0.1882 - acc: 0.9330 - val_loss: 0.1729 - val_acc: 0.9381\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17292 to 0.17223, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "9s - loss: 0.1703 - acc: 0.9397 - val_loss: 0.1722 - val_acc: 0.9361\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.17223 to 0.15203, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "9s - loss: 0.1576 - acc: 0.9435 - val_loss: 0.1520 - val_acc: 0.9445\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.15203 to 0.14397, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "9s - loss: 0.1484 - acc: 0.9463 - val_loss: 0.1440 - val_acc: 0.9455\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.14397 to 0.14088, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "9s - loss: 0.1407 - acc: 0.9493 - val_loss: 0.1409 - val_acc: 0.9465\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.14088 to 0.13240, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "10s - loss: 0.1342 - acc: 0.9510 - val_loss: 0.1324 - val_acc: 0.9498\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13240 to 0.13063, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "10s - loss: 0.1276 - acc: 0.9540 - val_loss: 0.1306 - val_acc: 0.9509\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.13063 to 0.12714, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "10s - loss: 0.1235 - acc: 0.9555 - val_loss: 0.1271 - val_acc: 0.9520\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: val_loss improved from 0.12714 to 0.12459, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "10s - loss: 0.1186 - acc: 0.9569 - val_loss: 0.1246 - val_acc: 0.9534\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12459 to 0.12415, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "10s - loss: 0.1143 - acc: 0.9591 - val_loss: 0.1242 - val_acc: 0.9526\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss improved from 0.12415 to 0.12314, saving model to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "9s - loss: 0.1111 - acc: 0.9599 - val_loss: 0.1231 - val_acc: 0.9532\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "9s - loss: 0.1073 - acc: 0.9621 - val_loss: 0.1416 - val_acc: 0.9447\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "9s - loss: 0.1041 - acc: 0.9628 - val_loss: 0.1280 - val_acc: 0.9510\n",
      "Epoch 00014: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_108_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.04525, saving model to ../models/cnn_attention_debias_tox_v2_108_probs_model.h5\n",
      "12s - loss: 0.0278 - acc: 0.9643 - val_loss: 0.0452 - val_acc: 0.9373\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.04525 to 0.03615, saving model to ../models/cnn_attention_debias_tox_v2_108_probs_model.h5\n",
      "9s - loss: 0.0266 - acc: 0.9662 - val_loss: 0.0361 - val_acc: 0.9508\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03615 to 0.03552, saving model to ../models/cnn_attention_debias_tox_v2_108_probs_model.h5\n",
      "9s - loss: 0.0255 - acc: 0.9676 - val_loss: 0.0355 - val_acc: 0.9531\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.03552 to 0.03449, saving model to ../models/cnn_attention_debias_tox_v2_108_probs_model.h5\n",
      "9s - loss: 0.0244 - acc: 0.9697 - val_loss: 0.0345 - val_acc: 0.9550\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.0233 - acc: 0.9714 - val_loss: 0.0379 - val_acc: 0.9496\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.0225 - acc: 0.9726 - val_loss: 0.0357 - val_acc: 0.9538\n",
      "Epoch 00005: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23128, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "14s - loss: 0.2987 - acc: 0.9058 - val_loss: 0.2313 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23128 to 0.18646, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "9s - loss: 0.2116 - acc: 0.9181 - val_loss: 0.1865 - val_acc: 0.9352\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18646 to 0.16535, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "9s - loss: 0.1772 - acc: 0.9369 - val_loss: 0.1653 - val_acc: 0.9414\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.16535 to 0.15261, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "9s - loss: 0.1590 - acc: 0.9427 - val_loss: 0.1526 - val_acc: 0.9449\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15261 to 0.13971, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "9s - loss: 0.1466 - acc: 0.9469 - val_loss: 0.1397 - val_acc: 0.9482\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.13971 to 0.13492, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "9s - loss: 0.1373 - acc: 0.9503 - val_loss: 0.1349 - val_acc: 0.9512\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.13492 to 0.13070, saving model to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "9s - loss: 0.1301 - acc: 0.9529 - val_loss: 0.1307 - val_acc: 0.9518\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.1249 - acc: 0.9551 - val_loss: 0.1314 - val_acc: 0.9509\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.1202 - acc: 0.9572 - val_loss: 0.1314 - val_acc: 0.9495\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_debias_tox_v2_109_model.h5\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03479, saving model to ../models/cnn_attention_debias_tox_v2_109_probs_model.h5\n",
      "12s - loss: 0.0321 - acc: 0.9585 - val_loss: 0.0348 - val_acc: 0.9552\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03479 to 0.03402, saving model to ../models/cnn_attention_debias_tox_v2_109_probs_model.h5\n",
      "9s - loss: 0.0308 - acc: 0.9601 - val_loss: 0.0340 - val_acc: 0.9549\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.03402 to 0.03325, saving model to ../models/cnn_attention_debias_tox_v2_109_probs_model.h5\n",
      "9s - loss: 0.0298 - acc: 0.9621 - val_loss: 0.0333 - val_acc: 0.9566\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0286 - acc: 0.9638 - val_loss: 0.0356 - val_acc: 0.9527\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.0276 - acc: 0.9652 - val_loss: 0.0342 - val_acc: 0.9542\n",
      "Epoch 00004: early stopping\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "for i in xrange(100, 110):\n",
    "\n",
    "    MODEL_NAME = 'cnn_attention_debias_tox_v2_{}'.format(i)\n",
    "    debias_model = AttentionToxModel()\n",
    "    debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.prep_data_and_score(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K \n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "# some selected passages of text from the wikipedia toxicity dataset\n",
    "input_texts = [\n",
    "        'Now go suck Cyphoidbomb\\'s transgenders dick if he has one.',\n",
    "        'Please relate the ozone hole to increases in cancer, and provide figures.',\n",
    "        'Is there any organisation that takes an impartial view in things, that did a mass study on gamergate and come to any solid conclusions about the nature of GamerGate? I\\'d love to see it.',\n",
    "        'This is fanaticism.  I suppose that next you will want to ``fix`` references to ``Christianity``']\n",
    "\n",
    "def run_graph(input_texts):\n",
    "    for input_text in input_texts:\n",
    "        activations = get_activations(model.probs_model, model.prep_text(input_text), \n",
    "                                      print_shape_only=True,layer_name=\"attention_vec\")\n",
    "        activations_flat = activations[0].flatten()\n",
    "        # 250 activations per character\n",
    "        display(activations_flat)\n",
    "        small = np.squeeze(activations_flat)[:len(input_text)]\n",
    "        plt.plot(range(len(input_text)),small)\n",
    "        plt.xticks(range(len(input_text)), [x for x in input_text])\n",
    "        plt.show()\n",
    "\n",
    "# run graphing\n",
    "run_graph(input_texts)\n",
    "# print the toxicity of each text\n",
    "print(model.predict(input_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
