{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Toxicity Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook trains a model to detect toxicity in online comments. It uses a CNN architecture for text classification trained on the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) and pre-trained GloVe embeddings which can be found at:\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "(source page: http://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "This model is a modification of [example code](https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py) found in the [Keras Github repository](https://github.com/fchollet/keras) and released under an [MIT license](https://github.com/fchollet/keras/blob/master/LICENSE). For further details of this license, find it [online](https://github.com/fchollet/keras/blob/master/LICENSE) or in this repository in the file KERAS_LICENSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "(TODO: nthain) - Move to README\n",
    "\n",
    "Prior to running the notebook, you must:\n",
    "\n",
    "* Download the [Wikipedia Talk Labels: Toxicity dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)\n",
    "* Download pre-trained [GloVe embeddings](http://nlp.stanford.edu/data/glove.6B.zip)\n",
    "* (optional) To skip the training step, you will need to download a model and tokenizer file. We are looking into the appropriate means for distributing these (sometimes large) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from model_tool import ToxModel\n",
    "from attention_model import AttentionToxModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__doc__', '__init__', '__module__', 'build_conv_layer', 'build_dense_attention_layer', 'build_model', 'fit_and_save_tokenizer', 'get_model_name', 'load_embeddings', 'load_model_from_name', 'predict', 'prep_text', 'print_hparams', 'save_hparams', 'score_auc', 'summary', 'train', 'update_hparams']\n"
     ]
    }
   ],
   "source": [
    "SPLITS = ['train', 'dev', 'test']\n",
    "\n",
    "wiki = {}\n",
    "debias = {}\n",
    "random = {}\n",
    "for split in SPLITS:\n",
    "    wiki[split] = '../data/wiki_%s.csv' % split\n",
    "    debias[split] = '../data/wiki_debias_%s.csv' % split\n",
    "    random[split] = '../data/wiki_debias_random_%s.csv' % split\n",
    "    \n",
    "print(dir(AttentionToxModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO from model_tool\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import cPickle\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "print('HELLO from model_tool')\n",
    "\n",
    "DEFAULT_EMBEDDINGS_PATH = '../data/glove.6B/glove.6B.100d.txt'\n",
    "DEFAULT_MODEL_DIR = '../models'\n",
    "\n",
    "DEFAULT_HPARAMS = {\n",
    "    'max_sequence_length': 250,\n",
    "    'max_num_words': 10000,\n",
    "    'embedding_dim': 100,\n",
    "    'embedding_trainable': False,\n",
    "    'learning_rate': 0.00005,\n",
    "    'stop_early': True,\n",
    "    'es_patience': 1, # Only relevant if STOP_EARLY = True\n",
    "    'es_min_delta': 0, # Only relevant if STOP_EARLY = True\n",
    "    'batch_size': 128,\n",
    "    'epochs': 20,\n",
    "    'dropout_rate': 0.3,\n",
    "    'cnn_filter_sizes': [128, 128, 128],\n",
    "    'cnn_kernel_sizes': [5,5,5],\n",
    "    'cnn_pooling_sizes': [5, 5, 40],\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "\n",
    "def compute_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "class ToxModel:\n",
    "    def __init__(self, \n",
    "                 model_name = None, \n",
    "                 model_dir = DEFAULT_MODEL_DIR,\n",
    "                 hparams = None):\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.hparams = DEFAULT_HPARAMS.copy()\n",
    "        if hparams:\n",
    "            self.update_hparams(hparams)\n",
    "        if model_name:\n",
    "            self.load_model_from_name(model_name)\n",
    "        self.print_hparams()\n",
    "\n",
    "    def print_hparams(self):\n",
    "        print('Hyperparameters')\n",
    "        print('---------------')\n",
    "        for k, v in self.hparams.iteritems():\n",
    "            print('{}: {}'.format(k, v))\n",
    "        print('')\n",
    "\n",
    "    def update_hparams(self, new_hparams):\n",
    "        self.hparams.update(new_hparams)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "    def save_hparams(self, model_name):\n",
    "        self.hparams['model_name'] = model_name\n",
    "        with open(os.path.join(self.model_dir, \n",
    "                '%s_hparams.json' % self.model_name), 'w') as f:\n",
    "            json.dump(self.hparams, f, sort_keys=True)\n",
    "\n",
    "    def load_model_from_name(self, model_name):\n",
    "        self.model = load_model(os.path.join(self.model_dir, '%s_model.h5' % model_name))\n",
    "        self.tokenizer = cPickle.load(open(os.path.join(self.model_dir, \n",
    "                                                        '%s_tokenizer.pkl' % model_name), \n",
    "                                           'rb'))\n",
    "        with open(os.path.join(self.model_dir, \n",
    "                '%s_hparams.json' % self.model_name), 'r') as f:\n",
    "            self.hparams = json.load(f)\n",
    "\n",
    "    def fit_and_save_tokenizer(self, texts):\n",
    "        \"\"\"Fits tokenizer on texts and pickles the tokenizer state.\"\"\"\n",
    "        self.tokenizer = Tokenizer(num_words = self.hparams['max_num_words'])\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        cPickle.dump(self.tokenizer, open(os.path.join(self.model_dir, '%s_tokenizer.pkl' % self.model_name), 'wb'))\n",
    "\n",
    "    def prep_text(self, texts):\n",
    "        \"\"\"Turns text into into padded sequences.\n",
    "\n",
    "        The tokenizer must be initialized before calling this method.\n",
    "\n",
    "        Args:\n",
    "            texts: Sequence of text strings.\n",
    "\n",
    "        Returns:\n",
    "            A tokenized and padded text sequence as a model input.\n",
    "        \"\"\"\n",
    "        text_sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(text_sequences, maxlen=self.hparams['max_sequence_length'])\n",
    "\n",
    "    def load_embeddings(self, embedding_path = DEFAULT_EMBEDDINGS_PATH):\n",
    "        embeddings_index = {}\n",
    "        with open(embedding_path) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index) + 1, self.hparams['embedding_dim']))\n",
    "        num_words_in_embedding = 0\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                num_words_in_embedding += 1\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    def train(self, training_data_path, validation_data_path, text_column, label_column, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.save_hparams(model_name)\n",
    "\n",
    "        train_data = pd.read_csv(training_data_path)\n",
    "        valid_data = pd.read_csv(validation_data_path)\n",
    "\n",
    "        print('Fitting tokenizer...')\n",
    "        self.fit_and_save_tokenizer(train_data[text_column])\n",
    "        print('Tokenizer fitted!')\n",
    "\n",
    "        print('Preparing data...')\n",
    "        train_text, train_labels = (self.prep_text(train_data[text_column]),\n",
    "                                    to_categorical(train_data[label_column]))\n",
    "        valid_text, valid_labels = (self.prep_text(valid_data[text_column]),\n",
    "                                    to_categorical(valid_data[label_column]))\n",
    "        print('Data prepared!')\n",
    "\n",
    "        print('Loading embeddings...')\n",
    "        self.load_embeddings()\n",
    "        print('Embeddings loaded!')\n",
    "\n",
    "        print('Building model graph...')\n",
    "        self.build_model()\n",
    "        print('Training model...')\n",
    "\n",
    "        save_path = os.path.join(self.model_dir, '%s_model.h5' % self.model_name)\n",
    "        callbacks = [ModelCheckpoint(save_path, save_best_only=True, verbose=self.hparams['verbose'])]\n",
    "\n",
    "        if self.hparams['stop_early']:\n",
    "            callbacks.append(EarlyStopping(min_delta=self.hparams['es_min_delta'],\n",
    "                monitor='val_loss', patience=self.hparams['es_patience'], verbose=self.hparams['verbose'], mode='auto'))\n",
    "\n",
    "        \n",
    "        self.model.fit(train_text,\n",
    "                       train_labels,\n",
    "                       batch_size=self.hparams['batch_size'],\n",
    "                       epochs=self.hparams['epochs'],\n",
    "                       validation_data=(valid_text, valid_labels),\n",
    "                       callbacks=callbacks,\n",
    "                       verbose=2)\n",
    "        \n",
    "        print('Model trained!')\n",
    "        print('Best model saved to {}'.format(save_path))\n",
    "        print('Loading best model from checkpoint...')\n",
    "        self.model = load_model(save_path)\n",
    "        print('Model loaded!')\n",
    "\n",
    "        if self.probs_model:\n",
    "            print('Fitting probs model')\n",
    "            save_path = os.path.join(self.model_dir, 'probs_model.h5')\n",
    "            callbacks = [ModelCheckpoint(save_path, save_best_only=True, verbose=self.hparams['verbose'])]\n",
    "\n",
    "            self.probs_model.fit(train_text,\n",
    "                       train_labels,\n",
    "                       batch_size=self.hparams['batch_size'],\n",
    "                       epochs=self.hparams['epochs'],\n",
    "                       validation_data=(valid_text, valid_labels),\n",
    "                       callbacks=callbacks,\n",
    "                       verbose=2)\n",
    "            \n",
    "            self.probs_model = load_model(save_path)\n",
    "            print('probs model loaded')\n",
    "            \n",
    "    def build_model(self):\n",
    "        sequence_input = Input(shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "        embedding_layer = Embedding(len(self.tokenizer.word_index) + 1,\n",
    "                                    self.hparams['embedding_dim'],\n",
    "                                    weights=[self.embedding_matrix],\n",
    "                                    input_length=self.hparams['max_sequence_length'],\n",
    "                                    trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        for filter_size, kernel_size, pool_size in zip(self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'], self.hparams['cnn_pooling_sizes']):\n",
    "            x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'])(x)\n",
    "        # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "        rmsprop = RMSprop(lr = self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=rmsprop,\n",
    "                      metrics=['acc'])\n",
    "                \n",
    "\n",
    "    def build_conv_layer(self, input_tensor, filter_size, kernel_size, pool_size):\n",
    "        output = Conv1D(filter_size, kernel_size, activation='relu', padding='same')(input_tensor)\n",
    "        if pool_size:\n",
    "            output = MaxPooling1D(pool_size, padding = 'same')(output)\n",
    "        else:\n",
    "            # TODO(nthain): This seems broken. Fix.\n",
    "            output = GlobalMaxPooling1D()(output)\n",
    "        return output\n",
    "\n",
    "    def predict(self, texts):\n",
    "        \"\"\"Returns model predictions on texts.\"\"\"\n",
    "        data = self.prep_text(texts)\n",
    "        return self.model.predict(data)[:,1]\n",
    "\n",
    "    def score_auc(self, texts, labels):\n",
    "        preds = self.predict(texts)\n",
    "        return compute_auc(labels, preds)\n",
    "\n",
    "\n",
    "    def summary():\n",
    "        return self.model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, merge, Multiply\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "attention_probs = None\n",
    "attention_mul = None\n",
    "attention_input = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "class AttentionToxModel(ToxModel):\n",
    "\n",
    "    def build_dense_attention_layer(self, input_tensor):\n",
    "        attention_input = input_tensor\n",
    "        attention_probs = Dense(self.hparams['max_sequence_length'], activation='softmax', name='attention_vec')(input_tensor)\n",
    "        attention_mul = Multiply()([input_tensor, attention_probs])\n",
    "        return {'attention_probs':attention_probs, 'attention_preds':attention_mul}\n",
    "\n",
    "    def build_probs(self):\n",
    "        sequence_input = Input(shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "        embedding_layer = Embedding(len(self.tokenizer.word_index) + 1,\n",
    "                                    self.hparams['embedding_dim'],\n",
    "                                    weights=[self.embedding_matrix],\n",
    "                                    input_length=self.hparams['max_sequence_length'],\n",
    "                                    trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        for filter_size, kernel_size, pool_size in zip(self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'], self.hparams['cnn_pooling_sizes']):\n",
    "            x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'], name=\"Dropout\")(x)\n",
    "        # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "        x = Dense(250, activation='relu', name=\"Dense_RELU\")(x)\n",
    "\n",
    "        attention_dict = self.build_dense_attention_layer(x)\n",
    "        preds = attention_dict['attention_probs']\n",
    "        preds = Dense(2, name=\"preds_dense\")(preds)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=rmsprop,metrics=['acc'])\n",
    "\n",
    "    def build_model(self):\n",
    "        print('print inside build model')\n",
    "        sequence_input = Input(shape=(self.hparams['max_sequence_length'],), dtype='int32')\n",
    "        embedding_layer = Embedding(len(self.tokenizer.word_index) + 1,\n",
    "                                    self.hparams['embedding_dim'],\n",
    "                                    weights=[self.embedding_matrix],\n",
    "                                    input_length=self.hparams['max_sequence_length'],\n",
    "                                    trainable=self.hparams['embedding_trainable'])\n",
    "\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "        x = embedded_sequences\n",
    "        for filter_size, kernel_size, pool_size in zip(self.hparams['cnn_filter_sizes'], self.hparams['cnn_kernel_sizes'], self.hparams['cnn_pooling_sizes']):\n",
    "            x = self.build_conv_layer(x, filter_size, kernel_size, pool_size)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(self.hparams['dropout_rate'], name=\"Dropout\")(x)\n",
    "        # TODO(nthain): Parametrize the number and size of fully connected layers\n",
    "        x = Dense(250, activation='relu', name=\"Dense_RELU\")(x)\n",
    "\n",
    "        attention_dict = self.build_dense_attention_layer(x)\n",
    "        preds = attention_dict['attention_preds']\n",
    "        preds = Dense(2, name=\"preds_dense\", activation='softmax')(preds)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.model = Model(sequence_input, preds)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=rmsprop,metrics=['acc'])\n",
    "\n",
    "        # now make probs model\n",
    "        probs = attention_dict['attention_probs']\n",
    "        probs = Dense(2, name='probs_dense')(probs)\n",
    "        rmsprop = RMSprop(lr=self.hparams['learning_rate'])\n",
    "        self.probs_model = Model(sequence_input, preds)\n",
    "        self.probs_model.compile(loss='mse', optimizer=rmsprop,metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Tox Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "embedding_dim: 100\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "es_patience: 1\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "batch_size: 128\n",
      "model_name: cnn_attention_random_tox_v4\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AttentionToxModel(model_name='cnn_attention_random_tox_v4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  is_toxic logged_in  \\\n",
      "0        == use of clown triggerfish ==  Dear Derek, ...     False     False   \n",
      "1      ` :::Regardless of whatever the supposed ``mai...     False      True   \n",
      "2      `  ==Wishaw General Hospital== A {{prod}} temp...     False      True   \n",
      "3       (UTC) * Flavour (particle physics) → Flavor (...     False      True   \n",
      "4                             ==SD.net VfD== Reverted.       False      True   \n",
      "5      `  == wanted: location of the theorem in Hamil...     False      True   \n",
      "6      `  ::::::::What you are missing is that we sim...     False      True   \n",
      "7       :You probably don't know it but you helped me...     False      True   \n",
      "8        The Washington Post is a reliable source, th...     False      True   \n",
      "9       Also I am party to someone's interview with W...     False      True   \n",
      "10     The last surviving Companion of the Order, Vic...     False       NaN   \n",
      "11      :.  It should not be added unless it is accom...     False      True   \n",
      "12     `By the way, thanks for giving me your IP addr...     False      True   \n",
      "13     `  ==Speedy deletion of Oliver Duché==  A tag ...     False      True   \n",
      "14                    ::Replied in new section there.        False      True   \n",
      "15      ::::Okay, I'll give it another whirl via emai...     False      True   \n",
      "16     William Deane (d.1818), a Fellow of All Souls ...     False       NaN   \n",
      "17     3  ==May 2013== Andy I understand what you sai...     False      True   \n",
      "18       ==Number of kills?==  The initial paragraph ...     False      True   \n",
      "19     , 10 July 2014 (UTC) :::::: ps: 13k hits for t...     False      True   \n",
      "20            REDIRECT Talk:Brian Smith (defensive end)      False      True   \n",
      "21                        ::Replied at that RfC page.        False      True   \n",
      "22     , 8 August 2009 (UTC)   I just saw Xeno's edit...     False     False   \n",
      "23       :To start: this cannot be discussed with you...     False      True   \n",
      "24     ` ::The venue point is unnecessary here as I p...     False      True   \n",
      "25      you bitch change that image on the 4 train li...      True     False   \n",
      "26     . I think it is a very important historical Hu...     False      True   \n",
      "27       Yeah, I realized I created a duplicate ID. S...     False      True   \n",
      "28     `  == Welcoming ==  Hm? Why couldn't someone w...     False      True   \n",
      "29     `  == Personal attacks ==  *, Making Personal ...     False     False   \n",
      "...                                                  ...       ...       ...   \n",
      "32991  ` ::::This is the letter from Birnbaum. I'd sa...     False      True   \n",
      "32992    ::The original RoC article (this) was writte...     False     False   \n",
      "32993  `  Well never mind, as it just happens i am bu...     False      True   \n",
      "32994  `  == use of ``states`` in the intro ==  The a...     False      True   \n",
      "32995   Mark Miller, I already told that to the edito...     False      True   \n",
      "32996    :Hi! Why I can see kingdoms Barghawata and S...     False      True   \n",
      "32997   :i don't have time to do it, but feel free to...     False      True   \n",
      "32998  `  :It is not listed on this European list as ...     False      True   \n",
      "32999   :::::*You have got to be kidding me. I am not...     False      True   \n",
      "33000  Any suggestions for appropriate punishments sh...     False      True   \n",
      "33001     == Injury ==  http://www.prowrestling.net/a...     False      True   \n",
      "33002   :If you have a source, please tell me the lin...     False      True   \n",
      "33003   After reading that, why should anyone believe...     False     False   \n",
      "33004    And this is the same issue as Kyiv/Kiev: gue...     False      True   \n",
      "33005    ==I've been nominated for admin==  Just thou...     False      True   \n",
      "33006  , 27 August 2007 (UTC)   Please stop. If you c...     False      True   \n",
      "33007  `  == ashleynn ==  I've watched  for a while. ...     False      True   \n",
      "33008  . Those descriptive terms are also accurate in...     False      True   \n",
      "33009           == Yay ==  Thank god I'm finally blocked     False     False   \n",
      "33010    == ???? ==  This 'list' serves no purpose ot...     False     False   \n",
      "33011     I think that there is some confusion in thi...     False     False   \n",
      "33012  `  To further clarify, even though ``club team...     False     False   \n",
      "33013  ` ::::No, I wrote that he ``may end up being d...     False      True   \n",
      "33014  `   Please do not vandalize pages, as you did ...     False      True   \n",
      "33015  ` :I will presume you meant nonsense, in that ...     False      True   \n",
      "33016  `  == Nope ==  There's little deleted history:...     False      True   \n",
      "33017    = fuck you, whores!!! I'LL FUCK YOU UP OLD-S...      True     False   \n",
      "33018    == Cthulhu in popular culture ==  FilkerTom ...     False      True   \n",
      "33019   :::  Hj108  that is exactly the point.  silen...     False      True   \n",
      "33020   :Well, what is the sentence trying to say? We...     False      True   \n",
      "\n",
      "            ns                                             rev_id  \\\n",
      "0         user                                        294546706.0   \n",
      "1      article                                        616830203.0   \n",
      "2         user                                        150939809.0   \n",
      "3      article                                         71969543.0   \n",
      "4         user                                         22001981.0   \n",
      "5      article                                        646996478.0   \n",
      "6      article                                        500401768.0   \n",
      "7      article                                        621770807.0   \n",
      "8         user                                        298495854.0   \n",
      "9      article                                        160744136.0   \n",
      "10         NaN  wikipedia_random546954_Order of the Star of In...   \n",
      "11     article                                        688977645.0   \n",
      "12     article                                        352046279.0   \n",
      "13        user                                        163189716.0   \n",
      "14        user                                        278060051.0   \n",
      "15        user                                        522504112.0   \n",
      "16         NaN  wikipedia_random52830678_Webbery, Alverdiscott_48   \n",
      "17        user                                        553942161.0   \n",
      "18     article                                        230199329.0   \n",
      "19        user                                        616365463.0   \n",
      "20     article                                        628024759.0   \n",
      "21        user                                        512048057.0   \n",
      "22     article                                        306736812.0   \n",
      "23     article                                        699483392.0   \n",
      "24        user                                        486340892.0   \n",
      "25        user                                        697127594.0   \n",
      "26     article                                        216371598.0   \n",
      "27        user                                        699698850.0   \n",
      "28        user                                        227293113.0   \n",
      "29     article                                         41106673.0   \n",
      "...        ...                                                ...   \n",
      "32991  article                                         42527143.0   \n",
      "32992  article                                        539458495.0   \n",
      "32993     user                                        530407523.0   \n",
      "32994  article                                        384362635.0   \n",
      "32995  article                                        623008346.0   \n",
      "32996  article                                         73763369.0   \n",
      "32997  article                                         69020260.0   \n",
      "32998  article                                         88268231.0   \n",
      "32999     user                                        140936839.0   \n",
      "33000  article                                         24908834.0   \n",
      "33001  article                                        259663288.0   \n",
      "33002  article                                        608982445.0   \n",
      "33003  article                                        414154236.0   \n",
      "33004  article                                         69091741.0   \n",
      "33005     user                                         63574893.0   \n",
      "33006     user                                        153936152.0   \n",
      "33007     user                                        353050123.0   \n",
      "33008     user                                        304813112.0   \n",
      "33009     user                                        191968699.0   \n",
      "33010  article                                         78299360.0   \n",
      "33011  article                                        161171678.0   \n",
      "33012  article                                        139174748.0   \n",
      "33013     user                                        383302180.0   \n",
      "33014     user                                        363576456.0   \n",
      "33015  article                                        278433053.0   \n",
      "33016     user                                          9445095.0   \n",
      "33017     user                                        199792170.0   \n",
      "33018  article                                        153423548.0   \n",
      "33019  article                                        354177934.0   \n",
      "33020  article                                        234768954.0   \n",
      "\n",
      "              sample split  toxicity    year  \n",
      "0             random  test       0.0  2009.0  \n",
      "1            blocked  test       0.0  2014.0  \n",
      "2             random  test       0.0  2007.0  \n",
      "3            blocked  test       0.0  2006.0  \n",
      "4            blocked  test       0.0  2005.0  \n",
      "5             random  test       0.0  2015.0  \n",
      "6            blocked  test       0.2  2012.0  \n",
      "7            blocked  test       0.0  2014.0  \n",
      "8             random  test       0.0  2009.0  \n",
      "9            blocked  test       0.1  2007.0  \n",
      "10     debias_random  test       NaN     NaN  \n",
      "11            random  test       0.0  2015.0  \n",
      "12           blocked  test       0.1  2010.0  \n",
      "13            random  test       0.0  2007.0  \n",
      "14            random  test       0.0  2009.0  \n",
      "15           blocked  test       0.0  2012.0  \n",
      "16     debias_random  test       NaN     NaN  \n",
      "17           blocked  test       0.1  2013.0  \n",
      "18            random  test       0.4  2008.0  \n",
      "19            random  test       0.2  2014.0  \n",
      "20            random  test       0.0  2014.0  \n",
      "21            random  test       0.0  2012.0  \n",
      "22           blocked  test       0.5  2009.0  \n",
      "23            random  test       0.0  2016.0  \n",
      "24           blocked  test       0.1  2012.0  \n",
      "25           blocked  test       0.9  2015.0  \n",
      "26           blocked  test       0.0  2008.0  \n",
      "27           blocked  test       0.0  2016.0  \n",
      "28            random  test       0.0  2008.0  \n",
      "29           blocked  test       0.3  2006.0  \n",
      "...              ...   ...       ...     ...  \n",
      "32991         random  test       0.1  2006.0  \n",
      "32992         random  test       0.1  2013.0  \n",
      "32993        blocked  test       0.0  2012.0  \n",
      "32994         random  test       0.0  2010.0  \n",
      "32995         random  test       0.0  2014.0  \n",
      "32996        blocked  test       0.0  2006.0  \n",
      "32997         random  test       0.0  2006.0  \n",
      "32998         random  test       0.0  2006.0  \n",
      "32999         random  test       0.1  2007.0  \n",
      "33000         random  test       0.2  2005.0  \n",
      "33001         random  test       0.0  2008.0  \n",
      "33002        blocked  test       0.1  2014.0  \n",
      "33003        blocked  test       0.2  2011.0  \n",
      "33004        blocked  test       0.0  2006.0  \n",
      "33005        blocked  test       0.0  2006.0  \n",
      "33006         random  test       0.1  2007.0  \n",
      "33007         random  test       0.2  2010.0  \n",
      "33008         random  test       0.0  2009.0  \n",
      "33009        blocked  test       0.3  2008.0  \n",
      "33010         random  test       0.3  2006.0  \n",
      "33011         random  test       0.0  2007.0  \n",
      "33012         random  test       0.0  2007.0  \n",
      "33013         random  test       0.0  2010.0  \n",
      "33014         random  test       0.0  2010.0  \n",
      "33015        blocked  test       0.3  2009.0  \n",
      "33016         random  test       0.0  2005.0  \n",
      "33017        blocked  test       1.0  2008.0  \n",
      "33018         random  test       0.0  2007.0  \n",
      "33019        blocked  test       0.0  2010.0  \n",
      "33020         random  test       0.0  2008.0  \n",
      "\n",
      "[33021 rows x 9 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92785887000837941"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "print(random_test)\n",
    "model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "print inside build model\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.23432, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.2997 - acc: 0.9056 - val_loss: 0.2343 - val_acc: 0.9078\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.23432 to 0.18941, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.2104 - acc: 0.9152 - val_loss: 0.1894 - val_acc: 0.9316\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.18941 to 0.17447, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1820 - acc: 0.9344 - val_loss: 0.1745 - val_acc: 0.9395\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.17447 to 0.15343, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1636 - acc: 0.9400 - val_loss: 0.1534 - val_acc: 0.9432\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.15343 to 0.14606, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1505 - acc: 0.9439 - val_loss: 0.1461 - val_acc: 0.9455\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.14606 to 0.13843, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1409 - acc: 0.9475 - val_loss: 0.1384 - val_acc: 0.9489\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.1337 - acc: 0.9500 - val_loss: 0.1410 - val_acc: 0.9475\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.13843 to 0.13149, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1268 - acc: 0.9529 - val_loss: 0.1315 - val_acc: 0.9520\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.13149 to 0.12488, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1215 - acc: 0.9543 - val_loss: 0.1249 - val_acc: 0.9532\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.1166 - acc: 0.9560 - val_loss: 0.1258 - val_acc: 0.9535\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.12488 to 0.12273, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1120 - acc: 0.9588 - val_loss: 0.1227 - val_acc: 0.9544\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.12273 to 0.12216, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1088 - acc: 0.9596 - val_loss: 0.1222 - val_acc: 0.9560\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "9s - loss: 0.1043 - acc: 0.9605 - val_loss: 0.1409 - val_acc: 0.9518\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss improved from 0.12216 to 0.12031, saving model to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "9s - loss: 0.1003 - acc: 0.9625 - val_loss: 0.1203 - val_acc: 0.9563\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "9s - loss: 0.0966 - acc: 0.9639 - val_loss: 0.1252 - val_acc: 0.9562\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "9s - loss: 0.0930 - acc: 0.9650 - val_loss: 0.1281 - val_acc: 0.9533\n",
      "Epoch 00015: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_attention_random_tox_v5_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n",
      "Fitting probs model\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.03497, saving model to ../models/probs_model.h5\n",
      "10s - loss: 0.0251 - acc: 0.9666 - val_loss: 0.0350 - val_acc: 0.9541\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.03497 to 0.03311, saving model to ../models/probs_model.h5\n",
      "9s - loss: 0.0238 - acc: 0.9689 - val_loss: 0.0331 - val_acc: 0.9581\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss did not improve\n",
      "9s - loss: 0.0224 - acc: 0.9713 - val_loss: 0.0335 - val_acc: 0.9565\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "9s - loss: 0.0212 - acc: 0.9732 - val_loss: 0.0342 - val_acc: 0.9570\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "9s - loss: 0.0200 - acc: 0.9750 - val_loss: 0.0681 - val_acc: 0.9001\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "9s - loss: 0.0191 - acc: 0.9763 - val_loss: 0.0365 - val_acc: 0.9537\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss did not improve\n",
      "9s - loss: 0.0178 - acc: 0.9786 - val_loss: 0.0359 - val_acc: 0.9549\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "9s - loss: 0.0167 - acc: 0.9799 - val_loss: 0.0360 - val_acc: 0.9570\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "9s - loss: 0.0159 - acc: 0.9810 - val_loss: 0.0396 - val_acc: 0.9504\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss did not improve\n",
      "9s - loss: 0.0152 - acc: 0.9822 - val_loss: 0.0398 - val_acc: 0.9514\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss did not improve\n",
      "9s - loss: 0.0148 - acc: 0.9825 - val_loss: 0.0387 - val_acc: 0.9559\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss did not improve\n",
      "9s - loss: 0.0136 - acc: 0.9843 - val_loss: 0.0382 - val_acc: 0.9560\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "9s - loss: 0.0131 - acc: 0.9850 - val_loss: 0.0385 - val_acc: 0.9555\n",
      "Epoch 14/20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = 'cnn_attention_random_tox_v5'\n",
    "debias_attention_model = AttentionToxModel()\n",
    "debias_attention_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_add_inbound_node', '_built', '_get_node_attribute_at_index', '_initial_weights', '_losses', '_node_key', '_non_trainable_weights', '_per_input_losses', '_per_input_updates', '_trainable_weights', '_updates', 'activation', 'activity_regularizer', 'add_loss', 'add_update', 'add_weight', 'assert_input_compatibility', 'bias', 'bias_constraint', 'bias_initializer', 'bias_regularizer', 'build', 'built', 'call', 'compute_mask', 'compute_output_shape', 'count_params', 'data_format', 'dilation_rate', 'filters', 'from_config', 'get_config', 'get_input_at', 'get_input_mask_at', 'get_input_shape_at', 'get_losses_for', 'get_output_at', 'get_output_mask_at', 'get_output_shape_at', 'get_updates_for', 'get_weights', 'inbound_nodes', 'input', 'input_mask', 'input_shape', 'input_spec', 'kernel', 'kernel_constraint', 'kernel_initializer', 'kernel_regularizer', 'kernel_size', 'losses', 'name', 'non_trainable_weights', 'outbound_nodes', 'output', 'output_mask', 'output_shape', 'padding', 'rank', 'set_weights', 'strides', 'supports_masking', 'trainable', 'trainable_weights', 'updates', 'use_bias', 'weights']\n",
      "[<tf.Variable 'conv1d_37_1/kernel:0' shape=(5, 100, 128) dtype=float32_ref>, <tf.Variable 'conv1d_37_1/bias:0' shape=(128,) dtype=float32_ref>]\n",
      "<bound method Conv1D.get_config of <keras.layers.convolutional.Conv1D object at 0x7f2f315c6e10>>\n",
      "[<keras.engine.topology.Node object at 0x7f2f315ced10>]\n",
      "Tensor(\"strided_slice:0\", shape=(250, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer = debias_attention_model.model.layers[2]\n",
    "print(dir(layer))\n",
    "print(layer.weights)\n",
    "print(layer.get_config)\n",
    "print(layer.outbound_nodes)\n",
    "print(layer.output[0])\n",
    "model = debias_attention_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e7356cdfab5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_auc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.17190, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "9s - loss: 0.2354 - acc: 0.9186 - val_loss: 0.1719 - val_acc: 0.9381\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.17190 to 0.14638, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1620 - acc: 0.9405 - val_loss: 0.1464 - val_acc: 0.9467\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.14638 to 0.13627, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1444 - acc: 0.9464 - val_loss: 0.1363 - val_acc: 0.9502\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.13627 to 0.12989, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1322 - acc: 0.9512 - val_loss: 0.1299 - val_acc: 0.9516\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "8s - loss: 0.1235 - acc: 0.9540 - val_loss: 0.1356 - val_acc: 0.9484\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.12989 to 0.12381, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1170 - acc: 0.9567 - val_loss: 0.1238 - val_acc: 0.9545\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.12381 to 0.12179, saving model to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "8s - loss: 0.1117 - acc: 0.9584 - val_loss: 0.1218 - val_acc: 0.9555\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss did not improve\n",
      "8s - loss: 0.1056 - acc: 0.9612 - val_loss: 0.1329 - val_acc: 0.9489\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss did not improve\n",
      "8s - loss: 0.1002 - acc: 0.9626 - val_loss: 0.1294 - val_acc: 0.9552\n",
      "Epoch 00008: early stopping\n",
      "Model trained!\n",
      "Best model saved to ../models/cnn_debias_random_tox_v3_model.h5\n",
      "Loading best model from checkpoint...\n",
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_debias_random_tox_v3'\n",
    "debias_random_model = ToxModel()\n",
    "debias_random_model.train(random['train'], random['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96087133494510768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_test = pd.read_csv(random['test'])\n",
    "debias_random_model.score_auc(random_test['comment'], random_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters\n",
      "---------------\n",
      "max_num_words: 10000\n",
      "dropout_rate: 0.3\n",
      "verbose: True\n",
      "cnn_pooling_sizes: [5, 5, 40]\n",
      "es_min_delta: 0\n",
      "learning_rate: 5e-05\n",
      "es_patience: 1\n",
      "batch_size: 128\n",
      "embedding_dim: 100\n",
      "epochs: 20\n",
      "cnn_filter_sizes: [128, 128, 128]\n",
      "cnn_kernel_sizes: [5, 5, 5]\n",
      "max_sequence_length: 250\n",
      "stop_early: True\n",
      "embedding_trainable: False\n",
      "\n",
      "Fitting tokenizer...\n",
      "Tokenizer fitted!\n",
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 95692 samples, validate on 32128 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.19048, saving model to ../models/cnn_wiki_tox_v3_model.h5\n",
      "8s - loss: 0.2477 - acc: 0.9105 - val_loss: 0.1905 - val_acc: 0.9340\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.19048 to 0.15330, saving model to ../models/cnn_wiki_tox_v3_model.h5\n",
      "8s - loss: 0.1690 - acc: 0.9383 - val_loss: 0.1533 - val_acc: 0.9431\n",
      "Epoch 3/20\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_wiki_tox_v3'\n",
    "wiki_model = ToxModel()\n",
    "wiki_model.train(wiki['train'], wiki['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_test = pd.read_csv(wiki['test'])\n",
    "wiki_model.score_auc(wiki_test['comment'], wiki_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debiased model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data prepared!\n",
      "Loading embeddings...\n",
      "Embeddings loaded!\n",
      "Building model graph...\n",
      "Training model...\n",
      "Train on 99157 samples, validate on 33283 samples\n",
      "Epoch 1/6\n",
      "99157/99157 [==============================] - 195s - loss: 0.2072 - acc: 0.9262 - val_loss: 0.1501 - val_acc: 0.9456\n",
      "Epoch 2/6\n",
      "99157/99157 [==============================] - 194s - loss: 0.1459 - acc: 0.9473 - val_loss: 0.1302 - val_acc: 0.9519\n",
      "Epoch 3/6\n",
      "99157/99157 [==============================] - 199s - loss: 0.1305 - acc: 0.9527 - val_loss: 0.1378 - val_acc: 0.9523\n",
      "Epoch 4/6\n",
      "99157/99157 [==============================] - 190s - loss: 0.1210 - acc: 0.9562 - val_loss: 0.1204 - val_acc: 0.9550\n",
      "Epoch 5/6\n",
      "99157/99157 [==============================] - 188s - loss: 0.1134 - acc: 0.9591 - val_loss: 0.1222 - val_acc: 0.9562\n",
      "Epoch 6/6\n",
      "99157/99157 [==============================] - 189s - loss: 0.1063 - acc: 0.9611 - val_loss: 0.1151 - val_acc: 0.9572\n",
      "<keras.callbacks.History object at 0x157ffb950>\n",
      "Model trained!\n",
      "Saving model...\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cnn_debias_tox_v3'\n",
    "debias_model = ToxModel()\n",
    "debias_model.train(debias['train'], debias['dev'], text_column = 'comment', label_column = 'is_toxic', model_name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97214632823959757"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debias_test = pd.read_csv(debias['test'])\n",
    "debias_model.prep_data_and_score(debias_test['comment'], debias_test['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
